import os

# å¦‚æœæ²¡è®¾ç½®ç¯å¢ƒå˜é‡ä¸è¦è®¾ç½®HF_ENDPOINTï¼ˆåˆ é™¤ä¸‹é¢ä¸‰è¡Œï¼‰ï¼Œä»å®˜ç½‘ä¸‹è½½æ¨¡å‹æˆ–è€…ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ–‡ä»¶?
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_OFFLINE"] = "0"
os.environ["TRANSFORMERS_OFFLINE"] = "0"

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "sk-FxhjDpv1D62n33JGICef3aVagezAr73GFnoXmSQ4ikMpf9Hb")
os.environ["OPENAI_API_URL"] = os.getenv("OPENAI_API_URL", "https://api.openai-proxy.org/v1")
os.environ["MODEL_NAME"] = os.getenv("MODEL_NAME", "gpt-4.1-nano")
# ä½¿ç”¨æœ¬åœ°SQLiteæ•°æ®åº“
os.environ["DB_PATH"] = os.getenv("DB_PATH", "store.db")

rag = None  # FastAPIå…¨å±€å˜é‡
import sqlite3  # ä½¿ç”¨SQLiteæ•°æ®åº“
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
from fastapi import FastAPI
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
from collections import deque
import re
import subprocess  # wh_add_draw
import sys  # wh_add_draw
import time  # wh_add_draw


class DatabaseSchemaAnalyzer:
    """åŠ¨æ€æ•°æ®åº“æ¨¡å¼åˆ†æå™¨ - æ”¯æŒSQLiteæ•°æ®åº“"""

    def __init__(self, conn):
        self.conn = conn
        self.schema_info = {}
        self.table_relationships = {}
        self.analyze_schema()

    def analyze_schema(self):
        """åˆ†ææ•°æ®åº“æ¨¡å¼ï¼Œè·å–æ‰€æœ‰è¡¨ã€å­—æ®µã€å…³ç³»"""
        cursor = self.conn.cursor()
        try:
            # 1. è·å–æ‰€æœ‰è¡¨
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = [row[0] for row in cursor.fetchall()]

            # 2. è·å–æ¯ä¸ªè¡¨çš„å­—æ®µä¿¡æ¯
            for table in tables:
                cursor.execute(f"PRAGMA table_info({table});")
                columns = cursor.fetchall()

                column_info = []
                for col in columns:
                    column_info.append({
                        'name': col[1],
                        'type': col[2],
                        'nullable': not col[3],  # SQLiteä¸­0è¡¨ç¤ºNOT NULL
                        'default': col[4]
                    })

                self.schema_info[table] = column_info

            # 3. åˆ†æå¤–é”®å…³ç³» (SQLiteå¤–é”®æ”¯æŒæœ‰é™)
            for table in tables:
                cursor.execute(f"PRAGMA foreign_key_list({table});")
                fks = cursor.fetchall()

                if table not in self.table_relationships:
                    self.table_relationships[table] = []

                for fk in fks:
                    self.table_relationships[table].append({
                        'column': fk[3],  # fromåˆ—
                        'foreign_table': fk[2],  # toè¡¨
                        'foreign_column': fk[4]  # toåˆ—
                    })

            print(f"âœ… æ•°æ®åº“æ¨¡å¼åˆ†æå®Œæˆï¼šå‘ç° {len(tables)} ä¸ªè¡¨")
            for table in tables:
                print(f"   ğŸ“‹ {table}: {len(self.schema_info[table])} ä¸ªå­—æ®µ")

        except Exception as e:
            print(f"âŒ æ•°æ®åº“æ¨¡å¼åˆ†æå¤±è´¥: {e}")
        finally:
            cursor.close()

    def get_schema_summary(self) -> str:
        """è·å–æ•°æ®åº“æ¨¡å¼æ‘˜è¦"""
        summary = []
        for table, columns in self.schema_info.items():
            col_names = [col['name'] for col in columns]
            summary.append(f"è¡¨ {table}: {', '.join(col_names)}")
        return "\n".join(summary)

    def find_related_tables(self, table_name: str) -> List[str]:
        """æŸ¥æ‰¾ä¸æŒ‡å®šè¡¨ç›¸å…³çš„è¡¨"""
        related = set()
        if table_name in self.table_relationships:
            for rel in self.table_relationships[table_name]:
                related.add(rel['foreign_table'])

        # åå‘æŸ¥æ‰¾
        for table, rels in self.table_relationships.items():
            for rel in rels:
                if rel['foreign_table'] == table_name:
                    related.add(table)

        return list(related)


class UniversalDatabaseAgent:
    """é€šç”¨æ•°æ®åº“Agent - æ”¯æŒSQLiteæ•°æ®åº“ç»“æ„"""

    def __init__(self):
        self.llm = ChatOpenAI(
            model_name=os.getenv("MODEL_NAME", "deepseek-chat"),
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=os.getenv("OPENAI_API_URL"),
            temperature=0.3
        )
        # è¿æ¥SQLiteæ•°æ®åº“
        db_path = os.getenv("DB_PATH", "store.db")
        self.conn = sqlite3.connect(db_path)
        self.schema_analyzer = DatabaseSchemaAnalyzer(self.conn)

    def generate_sql(self, question: str) -> Optional[str]:
        """ä½¿ç”¨LLMç”ŸæˆSQLæŸ¥è¯¢"""
        try:
            schema_summary = self.schema_analyzer.get_schema_summary()

            prompt = PromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚æ ¹æ®ä»¥ä¸‹æ•°æ®åº“æ¨¡å¼ï¼Œä¸ºç”¨æˆ·é—®é¢˜ç”ŸæˆSQLiteæŸ¥è¯¢è¯­å¥ã€‚
æ•°æ®åº“æ¨¡å¼ï¼š
{schema_summary}
ç”¨æˆ·é—®é¢˜ï¼š{question}
è¦æ±‚ï¼š
1. åªè¿”å›SQLè¯­å¥ï¼Œä¸è¦å…¶ä»–è§£é‡Š
2. ä½¿ç”¨LIMIT 10é™åˆ¶ç»“æœæ•°é‡
3. å¦‚æœæ¶‰åŠå¤šè¡¨ï¼Œä½¿ç”¨é€‚å½“çš„JOIN
4. ç¡®ä¿SQLè¯­æ³•æ­£ç¡®
5. å¦‚æœé—®é¢˜ä¸æ˜ç¡®ï¼Œè¿”å›NULL
SQLæŸ¥è¯¢ï¼š
""")

            response = self.llm.invoke(prompt.format(
                schema_summary=schema_summary,
                question=question
            ))

            sql = response.content.strip()
            if sql.upper().startswith('SELECT') and 'NULL' not in sql.upper():
                return sql
            return None

        except Exception as e:
            print(f"âŒ SQLç”Ÿæˆå¤±è´¥: {e}")
            return None

    def execute_query(self, sql: str) -> List[Tuple]:
        """æ‰§è¡ŒSQLæŸ¥è¯¢"""
        try:
            cursor = self.conn.cursor()
            cursor.execute(sql)
            rows = cursor.fetchall()
            cursor.close()
            return rows
        except Exception as e:
            print(f"âŒ SQLæ‰§è¡Œå¤±è´¥: {e}")
            return []

    def get_data_for_plotting(self, sql: str) -> Optional[List[Dict]]:  # wh_add_draw
        """æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è¿”å›å­—å…¸åˆ—è¡¨ï¼Œç”¨äºç»˜å›¾ã€‚"""  # wh_add_draw
        try:  # wh_add_draw
            cursor = self.conn.cursor()  # wh_add_draw
            cursor.execute(sql)  # wh_add_draw
            columns = [description[0] for description in cursor.description]  # wh_add_draw
            rows = cursor.fetchall()  # wh_add_draw
            cursor.close()  # wh_add_draw
            return [dict(zip(columns, row)) for row in rows]  # wh_add_draw
        except Exception as e:  # wh_add_draw
            print(f"âŒ SQLæ‰§è¡Œä»¥è·å–ç»˜å›¾æ•°æ®æ—¶å¤±è´¥: {e}")  # wh_add_draw
            return None  # wh_add_draw

    def analyze_results(self, question: str, rows: List[Tuple], sql: str) -> str:
        """åˆ†ææŸ¥è¯¢ç»“æœ"""
        if not rows:
            return "æœªæ‰¾åˆ°ç›¸å…³æ•°æ®"

        try:
            # å°†ç»“æœæ ¼å¼åŒ–ä¸ºæ–‡æœ¬
            result_text = "\n".join([str(row) for row in rows[:5]])  # åªæ˜¾ç¤ºå‰5è¡Œ

            prompt = PromptTemplate.from_template("""
åŸºäºä»¥ä¸‹æŸ¥è¯¢ç»“æœï¼Œä¸ºç”¨æˆ·é—®é¢˜æä¾›ä¸“ä¸šçš„ä¸šåŠ¡åˆ†æï¼š
ç”¨æˆ·é—®é¢˜ï¼š{question}
æ‰§è¡Œçš„SQLï¼š{sql}
æŸ¥è¯¢ç»“æœï¼š
{result_text}
è¯·æä¾›ï¼š
1. æ•°æ®æ¦‚è§ˆå’Œå…³é”®æŒ‡æ ‡
2. ä¸šåŠ¡æ´å¯Ÿå’Œå»ºè®®
3. æ•°æ®è¶‹åŠ¿åˆ†æï¼ˆå¦‚æœé€‚ç”¨ï¼‰
å›ç­”è¦ç®€æ´ä¸“ä¸šï¼Œä¸è¶…è¿‡200å­—ã€‚
""")

            response = self.llm.invoke(prompt.format(
                question=question,
                sql=sql,
                result_text=result_text
            ))

            return response.content

        except Exception as e:
            return f"ç»“æœåˆ†æå¤±è´¥: {str(e)}"

    def query(self, question: str, context: str = "") -> str:
        """é€šç”¨æ•°æ®åº“æŸ¥è¯¢æ¥å£"""
        try:
            # 1. ç”ŸæˆSQL
            sql = self.generate_sql(question)
            if not sql:
                return "æ— æ³•ç†è§£æŸ¥è¯¢éœ€æ±‚ï¼Œè¯·æä¾›æ›´å…·ä½“çš„é—®é¢˜"

            # 2. æ‰§è¡ŒæŸ¥è¯¢
            rows = self.execute_query(sql)

            # 3. åˆ†æç»“æœ
            analysis = self.analyze_results(question, rows, sql)

            return analysis

        except Exception as e:
            return f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}"

    def close(self):
        self.conn.close()


class InMemoryKnowledgeBase:
    def __init__(self):
        self.documents: List[Document] = []
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
        self.vectorstore = None

    def load_from_sqlite(self):
        """ä»SQLiteæ•°æ®åº“åŠ¨æ€åŠ è½½çŸ¥è¯†"""
        try:
            db_path = os.getenv("DB_PATH", "store.db")
            conn = sqlite3.connect(db_path)
            schema_analyzer = DatabaseSchemaAnalyzer(conn)

            # ä¸ºæ¯ä¸ªè¡¨ç”ŸæˆçŸ¥è¯†ç‰‡æ®µ
            for table_name, columns in schema_analyzer.schema_info.items():
                try:
                    # è·å–è¡¨çš„å‰50è¡Œæ•°æ®ä½œä¸ºç¤ºä¾‹
                    cursor = conn.cursor()
                    cursor.execute(f"SELECT * FROM {table_name} LIMIT 50")
                    rows = cursor.fetchall()

                    if rows:
                        # ç”Ÿæˆè¡¨ç»“æ„æè¿°
                        col_names = [col['name'] for col in columns]
                        table_desc = f"è¡¨ {table_name} åŒ…å«å­—æ®µï¼š{', '.join(col_names)}"
                        self.documents.append(Document(
                            page_content=table_desc,
                            metadata={"type": "table_schema", "table": table_name}
                        ))

                        # ç”Ÿæˆæ•°æ®ç¤ºä¾‹
                        for i, row in enumerate(rows[:3]):  # åªå–å‰3è¡Œ
                            data_desc = f"{table_name}è¡¨æ•°æ®ç¤ºä¾‹{i + 1}ï¼š{dict(zip(col_names, row))}"
                            self.documents.append(Document(
                                page_content=data_desc,
                                metadata={"type": "table_data", "table": table_name, "row": i + 1}
                            ))

                except Exception as e:
                    print(f"âš ï¸ å¤„ç†è¡¨ {table_name} æ—¶å‡ºé”™: {e}")
                    continue

            conn.close()
            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.documents)} ä¸ªæ•°æ®åº“çŸ¥è¯†ç‰‡æ®µ")
        except Exception as e:
            print(f"âŒ æ•°æ®åº“çŸ¥è¯†åŠ è½½å¤±è´¥: {e}")

    def build_vectorstore(self):
        if not self.documents:
            raise RuntimeError("æ²¡æœ‰çŸ¥è¯†ç‰‡æ®µå¯ç”¨äºå‘é‡åŒ–")
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = splitter.split_documents(self.documents)
        self.vectorstore = FAISS.from_documents(docs, self.embeddings)

    def cleanup(self):
        self.documents.clear()
        self.vectorstore = None


class MemoryAgent:
    """è®°å¿†Agent - è´Ÿè´£ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå¯¹è¯å†å²ç®¡ç†"""

    def __init__(self, max_memory_size=10):
        self.conversation_history = deque(maxlen=max_memory_size)
        self.context_summary = ""
        self.llm = ChatOpenAI(
            model_name=os.getenv("MODEL_NAME", "deepseek-chat"),
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=os.getenv("OPENAI_API_URL"),
            temperature=0.3
        )

    def add_interaction(self, question: str, answer: str, context: str = ""):
        """æ·»åŠ å¯¹è¯äº¤äº’åˆ°è®°å¿†"""
        interaction = {
            "question": question,
            "answer": answer,
            "context": context,
            "timestamp": len(self.conversation_history) + 1
        }
        self.conversation_history.append(interaction)
        self._update_context_summary()

    def _update_context_summary(self):
        """æ›´æ–°ä¸Šä¸‹æ–‡æ‘˜è¦"""
        if not self.conversation_history:
            self.context_summary = ""
            return

        try:
            recent_interactions = list(self.conversation_history)[-3:]  # æœ€è¿‘3æ¬¡äº¤äº’
            summary_prompt = PromptTemplate.from_template("""
åŸºäºä»¥ä¸‹æœ€è¿‘çš„å¯¹è¯å†å²ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œç”¨äºç†è§£ç”¨æˆ·çš„è¿ç»­é—®é¢˜ï¼š
å¯¹è¯å†å²ï¼š
{history}
è¯·ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œçªå‡ºå…³é”®ä¿¡æ¯å’Œç”¨æˆ·å…³æ³¨ç‚¹ï¼š
""")

            history_text = "\n".join([
                f"Q{i + 1}: {interaction['question']}\nA{i + 1}: {interaction['answer'][:100]}..."
                for i, interaction in enumerate(recent_interactions)
            ])

            response = self.llm.invoke(summary_prompt.format(history=history_text))
            self.context_summary = response.content.strip()

        except Exception as e:
            print(f"âš ï¸ ä¸Šä¸‹æ–‡æ‘˜è¦æ›´æ–°å¤±è´¥: {e}")
            self.context_summary = ""

    def get_context_for_query(self, current_question: str) -> str:
        """ä¸ºå½“å‰æŸ¥è¯¢è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        if not self.conversation_history:
            return ""
        # æ£€æŸ¥å½“å‰é—®é¢˜æ˜¯å¦æ¶‰åŠä¹‹å‰çš„ä¸Šä¸‹æ–‡
        context_keywords = ["ç»“åˆ", "åŸºäº", "æ ¹æ®", "ä¹‹å‰", "ä¸Šè¿°", "å‰é¢", "ç¬¬ä¸€ä¸ªé—®é¢˜"]
        has_context_reference = any(keyword in current_question for keyword in context_keywords)

        if has_context_reference and self.context_summary:
            return f"å¯¹è¯ä¸Šä¸‹æ–‡ï¼š{self.context_summary}\n"

        return ""

    def clear_memory(self):
        """æ¸…ç©ºè®°å¿†"""
        self.conversation_history.clear()
        self.context_summary = ""


class DrawingAgent:  # wh_add_draw
    """ç»˜å›¾Agent - è´Ÿè´£ç”Ÿæˆå¹¶æ‰§è¡Œç»˜å›¾ä»£ç """  # wh_add_draw

    def __init__(self):  # wh_add_draw
        self.llm = ChatOpenAI(  # wh_add_draw
            model_name=os.getenv("MODEL_NAME", "deepseek-chat"),  # wh_add_draw
            openai_api_key=os.getenv("OPENAI_API_KEY"),  # wh_add_draw
            openai_api_base=os.getenv("OPENAI_API_URL"),  # wh_add_draw
            temperature=0.4  # wh_add_draw
        )  # wh_add_draw

    def _extract_code(self, text: str) -> str:  # wh_add_draw
        """ä»æ–‡æœ¬ä¸­æå–Pythonä»£ç å—"""  # wh_add_draw
        if '```python' in text:  # wh_add_draw
            start = text.find('```python') + len('```python')  # wh_add_draw
            end = text.find('```', start)  # wh_add_draw
            return text[start:end].strip()  # wh_add_draw
        elif '```' in text:  # wh_add_draw
            start = text.find('```') + 3  # wh_add_draw
            end = text.find('```', start)  # wh_add_draw
            return text[start:end].strip()  # wh_add_draw
        return text  # wh_add_draw

    def draw(self, question: str, data_context: str = "") -> str:  # wh_add_draw
        """æ ¹æ®é—®é¢˜å’Œæ•°æ®ä¸Šä¸‹æ–‡ç”Ÿæˆå¹¶æ‰§è¡Œç»˜å›¾ä»£ç """  # wh_add_draw
        timestamp = int(time.time())  # wh_add_draw
        plot_filename = f"plot_{timestamp}.png"  # wh_add_draw
        plot_context = ""  # wh_add_draw
        if data_context:  # wh_add_draw
            plot_context = f"""
è¯·ä½¿ç”¨ä»¥ä¸‹JSONæ ¼å¼çš„æ•°æ®è¿›è¡Œç»˜å›¾ï¼Œä¸è¦è‡ªå·±ç¼–é€ æ•°æ®ï¼š
--- DATA START ---
{data_context}
--- DATA END ---
"""  # wh_add_draw
        plot_prompt_template = PromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªæ•°æ®å¯è§†åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œæä¾›çš„æ•°æ®ï¼Œç”Ÿæˆä¸€æ®µå®Œæ•´çš„Pythonä»£ç æ¥ç»˜åˆ¶å›¾è¡¨ã€‚

{plot_context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

ä»£ç è¦æ±‚ï¼š
1. ä½¿ç”¨ `matplotlib.pyplot` åº“ï¼Œå¹¶å°†å…¶åˆ«åä¸º `plt`ã€‚
2. **åœ¨è°ƒç”¨ `plt.show()` ä¹‹å‰ï¼Œå¿…é¡»å°†å›¾è¡¨ä¿å­˜åˆ°åä¸º '{plot_filename}' çš„æ–‡ä»¶ä¸­ã€‚**
3. **æœ€åå¿…é¡»è°ƒç”¨ `plt.show()` æ¥æ˜¾ç¤ºå›¾åƒã€‚**
4. ä»£ç å¿…é¡»æ˜¯å®Œæ•´ä¸”å¯ä»¥ç›´æ¥è¿è¡Œçš„ã€‚
5. ä½¿ç”¨è‹±æ–‡ä½œä¸ºå›¾è¡¨çš„æ ‡ç­¾ã€æ ‡é¢˜ï¼Œä»¥é¿å…ä¹±ç é—®é¢˜ã€‚
6. å¦‚æœæä¾›äº†æ•°æ®ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨æä¾›çš„æ•°æ®ã€‚å¦‚æœæ²¡æœ‰æä¾›ï¼Œå¯ä»¥ä½¿ç”¨åˆç†ã€ç®€æ´çš„ç¤ºä¾‹æ•°æ®ã€‚
7. ç»™å›¾è¡¨æ·»åŠ åˆé€‚çš„æ ‡é¢˜ (Title) å’Œåæ ‡è½´æ ‡ç­¾ (X/Y Label)ã€‚
8. åœ¨å›¾è¡¨åº•éƒ¨ä¸­å¿ƒä½ç½®æ·»åŠ æ³¨é‡Šï¼š'Note: Data is for reference only.'
9. åªè¿”å›Pythonä»£ç å—ï¼Œç”¨ ```python ... ``` åŒ…å›´ï¼Œä¸è¦ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚
""")  # wh_add_draw
        final_prompt = plot_prompt_template.format(question=question, plot_context=plot_context,
                                                   plot_filename=plot_filename)  # wh_add_draw
        attempt = 0  # wh_add_draw
        max_attempts = 5  # wh_add_draw
        conversation = [{"role": "system",
                         "content": "You are a helpful AI assistant that generates Python code for plotting graphs using matplotlib."}]  # wh_add_draw
        conversation.append({"role": "user", "content": final_prompt})  # wh_add_draw
        while attempt < max_attempts:  # wh_add_draw
            attempt += 1  # wh_add_draw
            print(f"\n[ç»˜å›¾å°è¯• {attempt}/{max_attempts}] æ­£åœ¨å‘LLMè¯·æ±‚ç»˜å›¾ä»£ç ...")  # wh_add_draw
            response = self.llm.invoke(conversation)  # wh_add_draw
            ai_response = response.content.strip()  # wh_add_draw
            code = self._extract_code(ai_response)  # wh_add_draw
            if not code:  # wh_add_draw
                print(f"âŒ ç»˜å›¾å¤±è´¥: LLMæœªè¿”å›æœ‰æ•ˆçš„ä»£ç ã€‚")  # wh_add_draw
                conversation.append({"role": "assistant", "content": ai_response})  # wh_add_draw
                conversation.append(
                    {"role": "user", "content": "ä½ æ²¡æœ‰è¿”å›ä»»ä½•ä»£ç ã€‚è¯·åªè¿”å›è¢«```pythonåŒ…å›´çš„ä»£ç å—ã€‚"})  # wh_add_draw
                continue  # wh_add_draw

            code = code.replace("matplotlib.use('Agg')", "")  # wh_add_draw
            code = code.replace("plt.show()", "")  # wh_add_draw
            code = re.sub(r"plt\.savefig\s*\(['\"].*?['\"]\)", "", code, flags=re.DOTALL)  # wh_add_draw
            code += f"\n\n# Adding save and show commands by the system #wh_add_draw\n"  # wh_add_draw
            code += f"plt.savefig('{plot_filename}') #wh_add_draw\n"  # wh_add_draw
            code += f"plt.show() #wh_add_draw\n"  # wh_add_draw

            script_name = f"temp_plot_{timestamp}_{attempt}.py"  # wh_add_draw
            with open(script_name, "w", encoding="utf-8") as f:  # wh_add_draw
                f.write(code)  # wh_add_draw
            try:  # wh_add_draw
                result = subprocess.run(  # wh_add_draw
                    [sys.executable, script_name],  # wh_add_draw
                    capture_output=True,  # wh_add_draw
                    text=True,  # wh_add_draw
                    timeout=30  # wh_add_draw
                )  # wh_add_draw
                if result.returncode == 0 and os.path.exists(plot_filename):  # wh_add_draw
                    print(f"âœ… ç»˜å›¾æˆåŠŸ! å›¾åƒå·²ä¿å­˜åˆ°: {os.path.abspath(plot_filename)}")  # wh_add_draw
                    os.remove(script_name)  # wh_add_draw
                    return f"ç»˜å›¾æˆåŠŸï¼Œæ–‡ä»¶ä¿å­˜åœ¨: {os.path.abspath(plot_filename)}"  # wh_add_draw
                else:  # wh_add_draw
                    error_msg = f"ä»£ç æ‰§è¡Œå¤±è´¥æˆ–æœªç”Ÿæˆå›¾åƒæ–‡ä»¶ã€‚\nReturn Code: {result.returncode}\nStderr: {result.stderr}"  # wh_add_draw
                    print(f"âŒ {error_msg}")  # wh_add_draw
                    conversation.append({"role": "assistant", "content": ai_response})  # wh_add_draw
                    feedback = f"ä½ ç”Ÿæˆçš„ä»£ç æ‰§è¡Œå¤±è´¥äº†ï¼Œé”™è¯¯ä¿¡æ¯æ˜¯: {error_msg}ã€‚è¯·ä¿®å¤å®ƒå¹¶é‡æ–°ç”Ÿæˆå®Œæ•´çš„ä»£ç ã€‚"  # wh_add_draw
                    conversation.append({"role": "user", "content": feedback})  # wh_add_draw
            except subprocess.TimeoutExpired:  # wh_add_draw
                error_msg = "æ‰§è¡Œè¶…æ—¶: ç»˜å›¾ä»£ç è¿è¡Œæ—¶é—´è¿‡é•¿ã€‚"  # wh_add_draw
                print(f"âŒ {error_msg}")  # wh_add_draw
                conversation.append({"role": "assistant", "content": ai_response})  # wh_add_draw
                conversation.append(
                    {"role": "user", "content": f"ä½ ç”Ÿæˆçš„ä»£ç æ‰§è¡Œè¶…æ—¶äº†ã€‚è¯·ä¼˜åŒ–ä»£ç ï¼Œä½¿å…¶èƒ½å¿«é€Ÿæ‰§è¡Œã€‚"})  # wh_add_draw
            except Exception as e:  # wh_add_draw
                error_msg = f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"  # wh_add_draw
                print(f"âŒ {error_msg}")  # wh_add_draw
                os.remove(script_name)  # wh_add_draw
                return f"ç»˜å›¾æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {error_msg}"  # wh_add_draw
            finally:  # wh_add_draw
                if os.path.exists(script_name):  # wh_add_draw
                    os.remove(script_name)  # wh_add_draw
        return f"âš ï¸ ç»è¿‡ {max_attempts} æ¬¡å°è¯•ï¼Œä»ç„¶æ— æ³•æˆåŠŸç”Ÿæˆå›¾åƒã€‚"  # wh_add_draw


class TopAgent:
    """TopAgent - ä½œä¸ºä¸­æ¢å¤§è„‘ï¼Œè´Ÿè´£ç†è§£ã€åˆ†æå’ŒAgentåè°ƒ"""

    def __init__(self, memory_agent: MemoryAgent, db_agent, kb, drawing_agent):  # wh_add_draw
        self.memory_agent = memory_agent
        self.db_agent = db_agent
        self.kb = kb
        self.drawing_agent = drawing_agent  # wh_add_draw
        self.llm = ChatOpenAI(
            model_name=os.getenv("MODEL_NAME", "deepseek-chat"),
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=os.getenv("OPENAI_API_URL"),
            temperature=0.3
        )
        # åˆå§‹åŒ–è¯­ä¹‰æ£€ç´¢ç»„ä»¶
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
        self.candidate_examples = self._initialize_candidate_examples()
        self.candidate_vectors = None
        self._build_candidate_vectors()

    def _initialize_candidate_examples(self) -> List[Dict]:
        """åˆå§‹åŒ–å€™é€‰ç¤ºä¾‹åº“"""
        examples = [
            {
                "task": "åº“å­˜åˆ†æ",
                "examples": [
                    "åˆ†æåº“å­˜ç‰©å“çš„ABCåˆ†ç±»",
                    "è®¡ç®—åº“å­˜å‘¨è½¬ç‡",
                    "è¯†åˆ«æ»é”€å•†å“",
                    "åˆ†æåº“å­˜ç»“æ„",
                    "è¯„ä¼°åº“å­˜æˆæœ¬"
                ]
            },
            {
                "task": "ä»“å‚¨è§„åˆ’",
                "examples": [
                    "ä¼˜åŒ–å­˜å‚¨ç­–ç•¥",
                    "è®¾è®¡è´§æ¶å¸ƒå±€",
                    "è§„åˆ’ä»“å‚¨ç©ºé—´",
                    "ç¡®å®šå­˜å‚¨ä½ç½®",
                    "åˆ†æå­˜å‚¨æ•ˆç‡"
                ]
            },
            {
                "task": "è®¢å•ç®¡ç†",
                "examples": [
                    "åˆ†æè®¢å•è¶‹åŠ¿",
                    "å¤„ç†è®¢å•å¼‚å¸¸",
                    "ä¼˜åŒ–è®¢å•æµç¨‹",
                    "ç»Ÿè®¡è®¢å•æ•°æ®",
                    "é¢„æµ‹è®¢å•é‡"
                ]
            },
            {
                "task": "ä¾›åº”é“¾åˆ†æ",
                "examples": [
                    "åˆ†æä¾›åº”å•†ç»©æ•ˆ",
                    "è¯„ä¼°ä¾›åº”é“¾é£é™©",
                    "ä¼˜åŒ–é‡‡è´­ç­–ç•¥",
                    "ç›‘æ§ä¾›åº”é“¾çŠ¶æ€",
                    "åˆ†æç‰©æµæˆæœ¬"
                ]
            },
            {
                "task": "æ•°æ®æŸ¥è¯¢",
                "examples": [
                    "æŸ¥è¯¢å•†å“ä¿¡æ¯",
                    "ç»Ÿè®¡é”€å”®æ•°æ®",
                    "åˆ†æå®¢æˆ·è¡Œä¸º",
                    "æŸ¥çœ‹åº“å­˜çŠ¶æ€",
                    "å¯¼å‡ºæŠ¥è¡¨æ•°æ®"
                ]
            }
        ]
        return examples

    def _build_candidate_vectors(self):
        """ç¦»çº¿æ„å»ºå€™é€‰ç¤ºä¾‹çš„å‘é‡è¡¨å¾"""
        try:
            all_examples = []
            for task_group in self.candidate_examples:
                for example in task_group["examples"]:
                    all_examples.append({
                        "text": example,
                        "task": task_group["task"],
                        "full_text": f"{task_group['task']}: {example}"
                    })
            # æ‰¹é‡ç”Ÿæˆå‘é‡è¡¨å¾
            texts = [item["full_text"] for item in all_examples]
            vectors = self.embeddings.embed_documents(texts)
            # å­˜å‚¨å‘é‡å’Œå…ƒæ•°æ®
            self.candidate_vectors = []
            for i, (item, vector) in enumerate(zip(all_examples, vectors)):
                self.candidate_vectors.append({
                    "id": i,
                    "text": item["text"],
                    "task": item["task"],
                    "full_text": item["full_text"],
                    "vector": vector
                })

            print(f"âœ… æˆåŠŸæ„å»º {len(self.candidate_vectors)} ä¸ªå€™é€‰ç¤ºä¾‹çš„å‘é‡è¡¨å¾")
        except Exception as e:
            print(f"âŒ å€™é€‰ç¤ºä¾‹å‘é‡æ„å»ºå¤±è´¥: {e}")
            self.candidate_vectors = []

    def _calculate_semantic_similarity(self, query_vector, candidate_vector) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰"""
        try:
            query_np = np.array(query_vector)
            candidate_np = np.array(candidate_vector)
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(query_np, candidate_np)
            query_norm = np.linalg.norm(query_np)
            candidate_norm = np.linalg.norm(candidate_np)
            if query_norm == 0 or candidate_norm == 0:
                return 0.0

            similarity = dot_product / (query_norm * candidate_norm)
            return float(similarity)

        except Exception as e:
            print(f"âš ï¸ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def _knn_semantic_search(self, query: str, k: int = 5) -> List[Dict]:
        """åŸºäºKNNçš„è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢"""
        if not self.candidate_vectors:
            return []
        try:
            # å®æ—¶è¡¨å¾ç”¨æˆ·è¾“å…¥
            query_vector = self.embeddings.embed_query(query)
            # è®¡ç®—ä¸æ‰€æœ‰å€™é€‰ç¤ºä¾‹çš„ç›¸ä¼¼åº¦
            similarities = []
            for candidate in self.candidate_vectors:
                similarity = self._calculate_semantic_similarity(query_vector, candidate["vector"])
                similarities.append({
                    "candidate": candidate,
                    "similarity": similarity
                })

            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰kä¸ª
            similarities.sort(key=lambda x: x["similarity"], reverse=True)
            top_k_results = similarities[:k]

            return top_k_results
        except Exception as e:
            print(f"âš ï¸ KNNè¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _enhance_query_with_semantic_context(self, query: str) -> str:
        """åŸºäºè¯­ä¹‰æ£€ç´¢å¢å¼ºæŸ¥è¯¢ä¸Šä¸‹æ–‡"""
        semantic_results = self._knn_semantic_search(query, k=3)

        if not semantic_results:
            return query

        # æ„å»ºè¯­ä¹‰ä¸Šä¸‹æ–‡
        context_parts = []
        for i, result in enumerate(semantic_results):
            candidate = result["candidate"]
            similarity = result["similarity"]
            if similarity > 0.5:  # åªä½¿ç”¨ç›¸ä¼¼åº¦è¾ƒé«˜çš„ç»“æœ
                context_parts.append(
                    f"ç›¸å…³ä»»åŠ¡{i + 1}: {candidate['task']} - {candidate['text']} (ç›¸ä¼¼åº¦: {similarity:.2f})")

        if context_parts:
            semantic_context = "\n".join(context_parts)
            enhanced_query = f"ç”¨æˆ·é—®é¢˜: {query}\n\nè¯­ä¹‰ç›¸å…³ä»»åŠ¡:\n{semantic_context}"
            return enhanced_query

        return query

    def analyze_query_intent(self, question: str, context: str = "") -> Dict:
        """åˆ†ææŸ¥è¯¢æ„å›¾ï¼Œå†³å®šéœ€è¦å“ªäº›Agentå‚ä¸"""
        try:
            intent_prompt = PromptTemplate.from_template("""
åˆ†æç”¨æˆ·é—®é¢˜çš„æ„å›¾ï¼Œå†³å®šéœ€è¦å“ªäº›ä¸“ä¸šAgentæ¥å›ç­”ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{question}
å¯¹è¯ä¸Šä¸‹æ–‡ï¼š{context}

è¯·åˆ†æé—®é¢˜ç±»å‹ï¼Œå¹¶è¿”å›JSONæ ¼å¼çš„å†³ç­–ï¼š
{{
    "requires_database": true/false,  // æ˜¯å¦éœ€è¦æ•°æ®åº“æŸ¥è¯¢è·å–æ•°æ®
    "requires_knowledge_base": true/false,  // æ˜¯å¦éœ€è¦çŸ¥è¯†åº“æ£€ç´¢
    "requires_drawing": true/false, // æ˜¯å¦éœ€è¦è°ƒç”¨ç»˜å›¾Agent
    "primary_agent": "database/knowledge_base/drawing/multi",  // ä¸»è¦Agent
    "reasoning": "åˆ†æç†ç”±"
}}

- å¦‚æœé—®é¢˜æ˜¯å…³äºâ€œç”»å›¾â€ã€â€œç»˜åˆ¶å›¾è¡¨â€ã€â€œå¯è§†åŒ–â€ç­‰ï¼Œè¯·è®¾ç½® "requires_drawing": true å¹¶ä¸” "primary_agent": "drawing"ã€‚
- å¦‚æœç»˜å›¾éœ€è¦æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æ•°æ®ï¼ˆå¦‚â€œç”»å‡ºæ¯ä¸ªä»“åº“çš„åº“å­˜é‡â€ï¼‰ï¼Œè¯·åŒæ—¶è®¾ç½® "requires_database": trueã€‚
- å…¶ä»–æƒ…å†µç…§å¸¸åˆ†æã€‚

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
""")  # wh_add_draw

            response = self.llm.invoke(intent_prompt.format(
                question=question,
                context=context
            ))

            # è§£æJSONå“åº”
            intent_data = json.loads(response.content.strip())
            return intent_data

        except Exception as e:
            if any(keyword in question for keyword in ["ç”»å›¾", "ç»˜åˆ¶", "plot", "draw", "å¯è§†åŒ–"]):  # wh_add_draw
                return {  # wh_add_draw
                    "requires_database": True,  # wh_add_draw
                    "requires_knowledge_base": False,  # wh_add_draw
                    "requires_drawing": True,  # wh_add_draw
                    "primary_agent": "drawing",  # wh_add_draw
                    "reasoning": "å…³é”®è¯è§¦å‘ç»˜å›¾æ¨¡å¼"  # wh_add_draw
                }  # wh_add_draw
            # é»˜è®¤è¿”å›å¤šAgentæ¨¡å¼
            return {
                "requires_database": True,
                "requires_knowledge_base": True,
                "requires_drawing": False,  # wh_add_draw
                "primary_agent": "multi",
                "reasoning": "é»˜è®¤å¤šAgentæ¨¡å¼"
            }

    def coordinate_agents(self, question: str, context: str = "") -> Dict:
        """åè°ƒå„ä¸ªAgentï¼Œè·å–ç»¼åˆå›ç­”"""
        if question.startswith("ç”»å›¾"):  # wh_add_draw
            print("INFO: æ£€æµ‹åˆ°'ç”»å›¾'å…³é”®è¯ï¼Œå¯åŠ¨ç»˜å›¾æµç¨‹ã€‚")  # wh_add_draw
            db_data_context = ""  # wh_add_draw
            print("INFO: ç»˜å›¾ä»»åŠ¡ï¼Œå°è¯•ä»æ•°æ®åº“è·å–ç›¸å…³æ•°æ®...")  # wh_add_draw
            sql = self.db_agent.generate_sql(question)  # wh_add_draw
            if sql:  # wh_add_draw
                plot_data = self.db_agent.get_data_for_plotting(sql)  # wh_add_draw
                if plot_data:  # wh_add_draw
                    db_data_context = json.dumps(plot_data, ensure_ascii=False, indent=2)  # wh_add_draw
                    print(f"INFO: æˆåŠŸè·å–æ•°æ®ä¸Šä¸‹æ–‡ç”¨äºç»˜å›¾:\n{db_data_context[:300]}...")  # wh_add_draw
                else:  # wh_add_draw
                    print("WARN: æœªèƒ½ä»æ•°æ®åº“è·å–ç”¨äºç»˜å›¾çš„æ•°æ®ã€‚å°†ä¾èµ–LLMç”Ÿæˆç¤ºä¾‹æ•°æ®ã€‚")  # wh_add_draw
            else:  # wh_add_draw
                print("WARN: æœªèƒ½ç”ŸæˆSQLæ¥è·å–ç»˜å›¾æ•°æ®ã€‚å°†ä¾èµ–LLMç”Ÿæˆç¤ºä¾‹æ•°æ®ã€‚")  # wh_add_draw
            plot_result = self.drawing_agent.draw(question, db_data_context)  # wh_add_draw
            return {  # wh_add_draw
                "answer": plot_result,  # wh_add_draw
                "knowledge_context": "",  # wh_add_draw
                "db_result": db_data_context,  # wh_add_draw
                "source_type": "drawing_agent",  # wh_add_draw
                "confidence": 0.95,  # wh_add_draw
                "agent_decision": {  # wh_add_draw
                    "primary_agent": "drawing",  # wh_add_draw
                    "reasoning": "ç”¨æˆ·è¾“å…¥ä»¥'ç”»å›¾'å¼€å¤´ï¼Œç›´æ¥è§¦å‘ã€‚",  # wh_add_draw
                    "requires_database": True,  # wh_add_draw
                    "requires_knowledge_base": False,  # wh_add_draw
                    "requires_drawing": True  # wh_add_draw
                },  # wh_add_draw
                "semantic_results": [],  # wh_add_draw
                "plot_path": plot_result if "æˆåŠŸ" in plot_result else None  # wh_add_draw
            }  # wh_add_draw

        # 1. è¯­ä¹‰æ£€ç´¢å¢å¼º
        enhanced_question = self._enhance_query_with_semantic_context(question)
        semantic_results = self._knn_semantic_search(question, k=3)
        # æ£€æŸ¥æœ€é«˜ç›¸å…³æ€§
        max_similarity = max([r['similarity'] for r in semantic_results], default=0)
        if max_similarity < 0.3:
            # ç›¸å…³æ€§ä½ï¼Œç›´æ¥ç”±å¤§æ¨¡å‹å›ç­”
            llm_prompt = PromptTemplate.from_template("""
ä½ æ˜¯æ™ºèƒ½ä»“å‚¨ç³»ç»Ÿçš„ä¸“å®¶ï¼Œè¯·ç›´æ¥ã€ä¸“ä¸šåœ°å›ç­”ä¸‹åˆ—ç”¨æˆ·é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ç”¨ç»“æ„åŒ–ã€ç®€æ˜çš„æ–¹å¼ä½œç­”ã€‚
""")
            answer = self.llm.invoke(llm_prompt.format(question=question)).content.strip()
            return {
                "answer": answer,
                "knowledge_context": "",
                "db_result": "",
                "source_type": "llm_fallback",
                "confidence": 0.7,
                "agent_decision": {
                    "primary_agent": "llm_fallback",
                    "reasoning": "è¯­ä¹‰ç›¸å…³æ€§ä½ï¼Œç›´æ¥ç”±å¤§æ¨¡å‹å›ç­”",
                    "requires_database": False,
                    "requires_knowledge_base": False
                },
                "semantic_results": semantic_results
            }

        # 2. åˆ†ææŸ¥è¯¢æ„å›¾
        try:
            intent = self.analyze_query_intent(enhanced_question, context)
        except Exception as e:
            intent = None
        # å¦‚æœæ„å›¾åˆ†æå¤±è´¥æˆ–è¿”å›ç©º/æ— æ•ˆï¼Œç›´æ¥ç”±LLMå›ç­”
        if not intent or not isinstance(intent, dict) or not intent.get('primary_agent'):
            llm_prompt = PromptTemplate.from_template("""
ä½ æ˜¯æ™ºèƒ½ä»“å‚¨ç³»ç»Ÿçš„ä¸“å®¶ï¼Œè¯·ç›´æ¥ã€ä¸“ä¸šåœ°å›ç­”ä¸‹åˆ—ç”¨æˆ·é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ç”¨ç»“æ„åŒ–ã€ç®€æ˜çš„æ–¹å¼ä½œç­”ã€‚
""")
            answer = self.llm.invoke(llm_prompt.format(question=question)).content.strip()
            return {
                "answer": answer,
                "knowledge_context": "",
                "db_result": "",
                "source_type": "llm_fallback",
                "confidence": 0.7,
                "agent_decision": {
                    "primary_agent": "llm_fallback",
                    "reasoning": "æ„å›¾åˆ†æå¤±è´¥æˆ–æ— æ•ˆï¼Œç›´æ¥ç”±å¤§æ¨¡å‹å›ç­”",
                    "requires_database": False,
                    "requires_knowledge_base": False
                },
                "semantic_results": semantic_results
            }

        # 3. æ ¹æ®æ„å›¾è°ƒç”¨ç›¸åº”Agent
        results = {}

        if intent.get("requires_knowledge_base", True):
            try:
                docs = self.kb.vectorstore.similarity_search(question, k=5)
                results["knowledge_context"] = self._format_knowledge_context(docs)
            except Exception as e:
                results["knowledge_context"] = f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}"

        if intent.get("requires_database", True):
            try:
                results["db_result"] = self.db_agent.query(question, context)
            except Exception as e:
                results["db_result"] = f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}"

        # 4. ç”Ÿæˆç»¼åˆå›ç­”
        final_answer = self._generate_comprehensive_answer(question, results, intent)

        return {
            "answer": final_answer,
            "knowledge_context": results.get("knowledge_context", ""),
            "db_result": results.get("db_result", ""),
            "source_type": "top_agent_coordinated",
            "confidence": 0.9,
            "agent_decision": intent,
            "semantic_results": semantic_results
        }

    def _format_knowledge_context(self, docs: List[Document]) -> str:
        """æ ¼å¼åŒ–çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼Œè§£å†³å¤šè¡Œéš”æ–­é—®é¢˜"""
        if not docs:
            return ""

        formatted_contexts = []
        for i, doc in enumerate(docs[:3]):  # åªå–å‰3ä¸ªæœ€ç›¸å…³çš„
            content = doc.page_content.strip()
            # æ¸…ç†å’Œæ ¼å¼åŒ–æ–‡æœ¬
            content = re.sub(r'\n+', ' ', content)  # å°†å¤šä¸ªæ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
            content = re.sub(r'\s+', ' ', content)  # å°†å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
            content = content[:300] + "..." if len(content) > 300 else content

            formatted_contexts.append(f"çŸ¥è¯†ç‰‡æ®µ{i + 1}: {content}")

        return "\n".join(formatted_contexts)

    def _generate_comprehensive_answer(self, question: str, results: Dict, intent: Dict) -> str:
        """ç”Ÿæˆç»¼åˆå›ç­”"""
        try:
            synthesis_prompt = PromptTemplate.from_template("""
ä½œä¸ºæ™ºèƒ½ä»“å‚¨ç³»ç»Ÿçš„ä¸­æ¢å¤§è„‘ï¼Œè¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸“ä¸šã€ç»“æ„åŒ–çš„ç»¼åˆå›ç­”ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{question}
Agentå†³ç­–ï¼š{intent_reasoning}

ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘
{knowledge_context}

ã€æ•°æ®åº“åˆ†æã€‘
{db_result}

è¯·æä¾›ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
2. åŸºäºå¤šæºä¿¡æ¯çš„ç»¼åˆåˆ†æ
3. å¦‚æœæœ‰ä¸Šä¸‹æ–‡å…³è”ï¼Œè¯·ä½“ç°è¿ç»­æ€§
4. å›ç­”è¦ç®€æ´ã€ä¸“ä¸šã€ç»“æ„åŒ–

ç»¼åˆå›ç­”ï¼š
""")

            response = self.llm.invoke(synthesis_prompt.format(
                question=question,
                intent_reasoning=intent.get("reasoning", ""),
                knowledge_context=results.get("knowledge_context", "æ— ç›¸å…³ä¿¡æ¯"),
                db_result=results.get("db_result", "æ— æ•°æ®åº“ç»“æœ")
            ))

            return response.content.strip()

        except Exception as e:
            return f"ç»¼åˆå›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}"


class AgenticRAGSystem:
    def __init__(self):
        self.kb = InMemoryKnowledgeBase()
        self.kb.load_from_sqlite()  # ä»SQLiteåŠ è½½çŸ¥è¯†
        self.kb.build_vectorstore()
        self.db_agent = UniversalDatabaseAgent()  # ä½¿ç”¨é€šç”¨æ•°æ®åº“Agent
        self.memory_agent = MemoryAgent()  # æ·»åŠ è®°å¿†Agent
        self.drawing_agent = DrawingAgent()  # wh_add_draw
        self.top_agent = TopAgent(self.memory_agent, self.db_agent, self.kb, self.drawing_agent)  # wh_add_draw
        self.llm = ChatOpenAI(
            model_name=os.getenv("MODEL_NAME"),
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=os.getenv("OPENAI_API_URL"),
            temperature=0.3
        )

    def process_query(self, query: str) -> Dict:
        # 1. è·å–ä¸Šä¸‹æ–‡
        context = self.memory_agent.get_context_for_query(query)

        # 2. TopAgentåè°ƒå„ä¸ªAgent
        result = self.top_agent.coordinate_agents(query, context)

        # 3. æ›´æ–°è®°å¿†
        self.memory_agent.add_interaction(query, result["answer"], context)

        return result

    def close(self):
        self.kb.cleanup()
        self.db_agent.close()
        self.memory_agent.clear_memory()  # æ¸…ç†è®°å¿†


# FastAPIæ¥å£
app = FastAPI(title="æ™ºèƒ½å¤šAgent RAG API")


class QueryRequest(BaseModel):
    question: str


@asynccontextmanager
async def lifespan(app: FastAPI):
    global rag
    rag = AgenticRAGSystem()
    yield
    rag.close()


app = FastAPI(lifespan=lifespan)


@app.post("/query")
def query_api(req: QueryRequest):
    try:
        result = rag.process_query(req.question)
        return result
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


def display_result(result: Dict):
    """æ ¼å¼åŒ–æ˜¾ç¤ºç»“æœ"""
    print("\n" + "=" * 50)
    print("ğŸ“ æ™ºèƒ½ç»“æ„åŒ–å›ç­”")
    print("=" * 50)
    print(result.get('answer', 'æ— å›ç­”'))

    if result.get("plot_path"):  # wh_add_draw
        print("âœ… å›¾åƒå·²å°è¯•åœ¨çª—å£ä¸­æ˜¾ç¤ºï¼Œå¹¶å·²ä¿å­˜åˆ°æœ¬åœ°ã€‚")  # wh_add_draw
    # ç½®ä¿¡åº¦å’Œç›¸å…³æ€§
    if 'confidence' in result:
        print(f"\nğŸ¯ ç½®ä¿¡åº¦: {result['confidence']:.1%}")
    if 'relevance_score' in result:
        print(f"ğŸ“Š çŸ¥è¯†åº“åŒ¹é…åº¦: {result['relevance_score']:.1%}")

    # è¯­ä¹‰æ£€ç´¢ç»“æœ
    if 'semantic_results' in result and result['semantic_results']:
        print("\nğŸ” è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢")
        print("-" * 20)
        for i, semantic_result in enumerate(result['semantic_results'][:3]):
            candidate = semantic_result['candidate']
            similarity = semantic_result['similarity']
            if similarity > 0.3:  # åªæ˜¾ç¤ºç›¸ä¼¼åº¦è¾ƒé«˜çš„ç»“æœ
                print(f"ç›¸å…³ä»»åŠ¡{i + 1}: {candidate['task']} - {candidate['text']}")
                print(f"ç›¸ä¼¼åº¦: {similarity:.3f}")

    # Agentå†³ç­–ä¿¡æ¯
    if 'agent_decision' in result:
        print("\nğŸ¤– Agentå†³ç­–åˆ†æ")
        print("-" * 20)
        decision = result['agent_decision']
        print(f"ä¸»è¦Agent: {decision.get('primary_agent', 'unknown')}")
        print(f"åˆ†æç†ç”±: {decision.get('reasoning', 'æ— ')}")
        print(f"æ•°æ®åº“æŸ¥è¯¢: {'âœ…' if decision.get('requires_database') else 'âŒ'}")
        print(f"çŸ¥è¯†åº“æ£€ç´¢: {'âœ…' if decision.get('requires_knowledge_base') else 'âŒ'}")

    # ä¿¡æ¯æ¥æº
    print("\nğŸ“‹ ä¿¡æ¯æ¥æº")
    print("-" * 20)
    source_type = result.get('source_type', 'unknown')
    if source_type == "drawing_agent":  # wh_add_draw
        print("ğŸ¨ ç»˜å›¾Agent")  # wh_add_draw
    elif source_type == "top_agent_coordinated":
        print("ğŸ§  TopAgentåè°ƒç³»ç»Ÿ")
    elif source_type == "knowledge_base":
        print("âœ… çŸ¥è¯†åº“ç›´æ¥åŒ¹é…")
    elif source_type == "agent_system":
        print("ğŸ¤– å¤šAgentåè°ƒç³»ç»Ÿ")
    elif source_type == "llm_fallback":
        print("âš ï¸ LLMç›´æ¥ç”Ÿæˆ")
    else:
        print("â“ æœªçŸ¥æ¥æº")

    # è¯¦ç»†ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
    if 'knowledge_context' in result and result['knowledge_context']:
        print("\nğŸ§  çŸ¥è¯†åº“ç‰‡æ®µ:")
        print(result['knowledge_context'])
    if 'db_result' in result and result['db_result']:
        print("\nğŸ’¾ æ•°æ®åº“åˆ†æ:")
        print(result['db_result'])


def main():
    print("ğŸš€ === æ™ºèƒ½å¤šAgent RAGä»“åº“ç®¡ç†ç³»ç»Ÿï¼ˆSQLiteç‰ˆï¼‰ ===")
    print("ğŸ’¡ è¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢ï¼š")
    print("ğŸ”š è¾“å…¥'é€€å‡º'ã€'quit'ã€'exit'æˆ–'q'ç»“æŸä¼šè¯")
    print("ğŸ§¹ è¾“å…¥'clear'æ¸…ç©ºå¯¹è¯è®°å¿†\n")

    rag = AgenticRAGSystem()
    try:
        while True:
            query = input("\nğŸ¤” è¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢> ").strip()
            if not query:
                continue
            if query.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ§¹ æ­£åœ¨æ¸…ç©ºå¯¹è¯è®°å¿†...")
                rag.memory_agent.clear_memory()
                print("âœ… å¯¹è¯è®°å¿†å·²æ¸…ç©º")
                break
            if query.lower() == 'clear':
                rag.memory_agent.clear_memory()
                print("ğŸ§¹ å¯¹è¯è®°å¿†å·²æ¸…ç©º")
                continue
            result = rag.process_query(query)
            display_result(result)
    finally:
        rag.close()
        print("\nğŸ‘‹ ç³»ç»Ÿå·²å…³é—­")


# å‘½ä»¤è¡Œäº¤äº’
if __name__ == "__main__":
    main()