import os#è¿è¡Œéœ€è¦1åˆ†é’Ÿå·¦å³ï¼Œå›ç­”15-30ç§’å·¦å³
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_OFFLINE"] = "0"
os.environ["TRANSFORMERS_OFFLINE"] = "0"
#sk-FxhjDpv1D62n33JGICef3aVagezAr73GFnoXmSQ4ikMpf9Hb ï¼›sk-tgq6Xw43DMpw510JMGFofD8UPoBZTRUSrtoywgnbIdx8Z88X
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "sk-tgq6Xw43DMpw510JMGFofD8UPoBZTRUSrtoywgnbIdx8Z88X")#å…¶ä»–apiå¯†é’¥ç›´æ¥æ”¹è¿™é‡Œï¼Œå¦‚æœcloseaiçš„æ¬ è´¹äº†ç”¨è¿™ä¸ªå¯†é’¥ï¼šsk-tgq6Xw43DMpw510JMGFofD8UPoBZTRUSrtoywgnbIdx8Z88X
os.environ["OPENAI_API_URL"] = os.getenv("OPENAI_API_URL", "https://api.openai-proxy.org/v1")
os.environ["MODEL_NAME"] = os.getenv("MODEL_NAME", "gpt-4.1")#ä½¿ç”¨çš„æ˜¯closeai çš„(  gpt-4.1-nano/deepseek-chat  )æ¨¡å‹
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"

rag = None  # FastAPIå…¨å±€å˜é‡
import psycopg2
import fitz
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
from fastapi import FastAPI, Request
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
from collections import deque
import re
import textwrap
import subprocess  # æ·»åŠ ç»˜å›¾åŠŸèƒ½
import sys  # æ·»åŠ ç»˜å›¾åŠŸèƒ½
import time  # æ·»åŠ ç»˜å›¾åŠŸèƒ½
# PostgreSQLé…ç½®
PG_HOST = os.getenv('PG_HOST', '192.168.28.135')
PG_PORT = os.getenv('PG_PORT', '5432')
PG_NAME = os.getenv('PG_NAME', 'companylink')
PG_USER = os.getenv('PG_USER', 'myuser')
PG_PASSWORD = os.getenv('PG_PASSWORD', '123456abc.')

#æœ¬åœ°çŸ¥è¯†åº“æ‰€éœ€è¦pdfæ–‡ä»¶è·¯å¾„
PDF_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'knowledge_pdfs')

class DatabaseSchemaAnalyzer:
    """åŠ¨æ€æ•°æ®åº“æ¨¡å¼åˆ†æå™¨ - æ”¯æŒä»»ä½•PostgreSQLæ•°æ®åº“"""    
    def __init__(self, conn):
        self.conn = conn
        self.schema_info = {}
        self.table_relationships = {}
        self.analyze_schema()
    
    def analyze_schema(self):
        """åˆ†ææ•°æ®åº“æ¨¡å¼ï¼Œè·å–æ‰€æœ‰è¡¨ã€å­—æ®µã€å…³ç³»"""
        cursor = self.conn.cursor()
        try:
            # 1. è·å–æ‰€æœ‰è¡¨
            cursor.execute("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """)
            tables = [row[0] for row in cursor.fetchall()]
            
            # 2. è·å–æ¯ä¸ªè¡¨çš„å­—æ®µä¿¡æ¯
            for table in tables:
                cursor.execute("""
                    SELECT column_name, data_type, is_nullable, column_default
                    FROM information_schema.columns 
                    WHERE table_name = %s 
                    AND table_schema = 'public'
                    ORDER BY ordinal_position
                """, (table,))
                
                columns = []
                for row in cursor.fetchall():
                    columns.append({
                        'name': row[0],
                        'type': row[1],
                        'nullable': row[2] == 'YES',
                        'default': row[3]
                    })
                
                self.schema_info[table] = columns
            
            # 3. åˆ†æå¤–é”®å…³ç³»
            cursor.execute("""
                SELECT 
                    tc.table_name, 
                    kcu.column_name, 
                    ccu.table_name AS foreign_table_name,
                    ccu.column_name AS foreign_column_name 
                FROM information_schema.table_constraints AS tc 
                JOIN information_schema.key_column_usage AS kcu
                    ON tc.constraint_name = kcu.constraint_name
                    AND tc.table_schema = kcu.table_schema
                JOIN information_schema.constraint_column_usage AS ccu
                    ON ccu.constraint_name = tc.constraint_name
                    AND ccu.table_schema = tc.table_schema
                WHERE tc.constraint_type = 'FOREIGN KEY' 
                AND tc.table_schema = 'public'
            """)
            
            for row in cursor.fetchall():
                table, column, foreign_table, foreign_column = row
                if table not in self.table_relationships:
                    self.table_relationships[table] = []
                self.table_relationships[table].append({
                    'column': column,
                    'foreign_table': foreign_table,
                    'foreign_column': foreign_column
                })
            
        #    print(f"âœ… æ•°æ®åº“æ¨¡å¼åˆ†æå®Œæˆï¼šå‘ç° {len(tables)} ä¸ªè¡¨")
        #   for table in tables:
        #       print(f"   ğŸ“‹ {table}: {len(self.schema_info[table])} ä¸ªå­—æ®µ")
                
        except Exception as e:
            print(f"âŒ æ•°æ®åº“æ¨¡å¼åˆ†æå¤±è´¥: {e}")
        finally:
            cursor.close()
    
    def get_schema_summary(self) -> str:
        """è·å–æ•°æ®åº“æ¨¡å¼æ‘˜è¦"""
        summary = []
        for table, columns in self.schema_info.items():
            col_names = [col['name'] for col in columns]
            summary.append(f"è¡¨ {table}: {', '.join(col_names)}")
        return "\n".join(summary)
    
    def find_related_tables(self, table_name: str) -> List[str]:
        """æŸ¥æ‰¾ä¸æŒ‡å®šè¡¨ç›¸å…³çš„è¡¨"""
        related = set()
        if table_name in self.table_relationships:
            for rel in self.table_relationships[table_name]:
                related.add(rel['foreign_table'])
        
        # åå‘æŸ¥æ‰¾
        for table, rels in self.table_relationships.items():
            for rel in rels:
                if rel['foreign_table'] == table_name:
                    related.add(table)
        
        return list(related)

class UniversalDatabaseAgent:
    """é€šç”¨æ•°æ®åº“Agent - æ”¯æŒä»»ä½•PostgreSQLæ•°æ®åº“ç»“æ„"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model_name=os.getenv("MODEL_NAME", "gpt-4.1"),
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=os.getenv("OPENAI_API_URL"),
            temperature=0.3
        )
        self.conn = psycopg2.connect(
            host=PG_HOST, port=PG_PORT, database=PG_NAME, user=PG_USER, password=PG_PASSWORD
        )
        self.schema_analyzer = DatabaseSchemaAnalyzer(self.conn)
    
    def get_data_for_plotting(self, sql: str) -> Optional[List[Dict]]:
        """æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è¿”å›å­—å…¸åˆ—è¡¨ï¼Œç”¨äºç»˜å›¾"""
        try:
            cursor = self.conn.cursor()
            cursor.execute(sql)
            columns = [description[0] for description in cursor.description]
            rows = cursor.fetchall()
            cursor.close()
            return [dict(zip(columns, row)) for row in rows]
        except Exception as e:
            import traceback
            print(f"âŒ SQLæ‰§è¡Œä»¥è·å–ç»˜å›¾æ•°æ®æ—¶å¤±è´¥: {e}")
            print(f"SQLè¯­å¥: {sql}")
            traceback.print_exc()
            return None
    
    def analyze_query_intent(self, question: str) -> Dict:
        """æ™ºèƒ½åˆ†æç”¨æˆ·æŸ¥è¯¢æ„å›¾ï¼Œå°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºæ•°æ®åº“æŸ¥è¯¢éœ€æ±‚"""
        try:
            print(f"ğŸ§  å¼€å§‹åˆ†ææŸ¥è¯¢æ„å›¾: {question}")
            
            # è·å–æ•°æ®åº“æ¨¡å¼ä¿¡æ¯
            schema_summary = self.schema_analyzer.get_schema_summary()
            
                                      # æ„å»ºæ„å›¾åˆ†ææç¤º
            intent_prompt = PromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åº“æŸ¥è¯¢æ„å›¾åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œå°†å…¶è½¬æ¢ä¸ºå…·ä½“çš„æ•°æ®åº“æŸ¥è¯¢éœ€æ±‚ã€‚

æ•°æ®åº“æ¨¡å¼ä¿¡æ¯ï¼š
{schema_summary}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åˆ†æç”¨æˆ·æ„å›¾å¹¶è¿”å›JSONæ ¼å¼çš„æŸ¥è¯¢éœ€æ±‚ï¼š
{{
    "query_type": "é”€å”®åˆ†æ/åº“å­˜åˆ†æ/äº§å“åˆ†æ/ä»“åº“åˆ†æ/é—¨åº—åˆ†æ/è¡¥è´§åˆ†æ/è¶‹åŠ¿åˆ†æ/ç»¼åˆæŸ¥è¯¢",
    "target_tables": ["è¡¨å1", "è¡¨å2"],
    "target_columns": ["å­—æ®µ1", "å­—æ®µ2"],
    "filter_conditions": {{
        "category": "äº§å“ç±»åˆ«ï¼ˆå¦‚ï¼šELECTRONICS, BEVERAGE, SNACKç­‰ï¼‰",
        "location": "ä½ç½®ä¿¡æ¯ï¼ˆå¦‚ï¼šä»“åº“IDã€é—¨åº—IDï¼‰",
        "time_range": "æ—¶é—´èŒƒå›´",
        "product_name": "äº§å“åç§°å…³é”®è¯",
        "warehouse_name": "ä»“åº“åç§°å…³é”®è¯",
        "store_name": "é—¨åº—åç§°å…³é”®è¯"
    }},
    "aggregation": {{
        "functions": ["SUM", "COUNT", "AVG", "MAX", "MIN"],
        "group_by": ["åˆ†ç»„å­—æ®µ"],
        "order_by": ["æ’åºå­—æ®µ"]
    }},
    "business_insight": "ä¸šåŠ¡æ´å¯Ÿéœ€æ±‚",
    "confidence": 0.0-1.0
}}

æ™ºèƒ½è¯†åˆ«è§„åˆ™ï¼š

1. äº§å“ç±»åˆ«æ˜ å°„ï¼ˆå¿…é¡»å‡†ç¡®è¯†åˆ«ï¼Œä¸”categoryå€¼å¿…é¡»ä¸æ•°æ®åº“å­—æ®µå®Œå…¨ä¸€è‡´ï¼ŒåŒºåˆ†å¤§å°å†™ã€‚ä¾‹å¦‚ï¼šELECTRONICSã€BEVERAGEã€SNACKã€DAILYã€FROZENã€APPLIANCEç­‰ï¼‰ï¼š
   - "ç”µå­äº§å“"ã€"ç”µå­"ã€"æ•°ç "ã€"æ‰‹æœº"ã€"ç”µè„‘"ã€"è€³æœº"ã€"å……ç”µå®"ã€"iPhone"ã€"åä¸º"ã€"å°ç±³" â†’ category: "ELECTRONICS"
   - "é¥®æ–™"ã€"çŸ¿æ³‰æ°´"ã€"å¯ä¹"ã€"ç‰›å¥¶"ã€"èŒ¶"ã€"å’–å•¡"ã€"çº¢ç‰›"ã€"ä¼Šåˆ©" â†’ category: "BEVERAGE" 
   - "é›¶é£Ÿ"ã€"è–¯ç‰‡"ã€"å·§å…‹åŠ›"ã€"é¥¼å¹²"ã€"åšæœ"ã€"ç³–æœ"ã€"ä¹äº‹"ã€"å¾·èŠ™"ã€"å¥¥åˆ©å¥¥" â†’ category: "SNACK"
   - "æ—¥ç”¨å“"ã€"æ´—å‘æ°´"ã€"ç‰™è†"ã€"é¦™çš‚"ã€"çº¸å·¾"ã€"æ´—è¡£æ¶²"ã€"æµ·é£ä¸"ã€"ä½³æ´å£«"ã€"èˆ’è‚¤ä½³" â†’ category: "DAILY"
   - "å†·å†»é£Ÿå“"ã€"å†°æ·‡æ·‹"ã€"æ°´é¥º"ã€"ç‰›æ’"ã€"æ±¤åœ†"ã€"é€Ÿå†»"ã€"æ¹¾ä»”ç å¤´"ã€"å“ˆæ ¹è¾¾æ–¯" â†’ category: "FROZEN"
   - "å®¶ç”µ"ã€"ç©ºè°ƒ"ã€"å†°ç®±"ã€"å¸å°˜å™¨"ã€"ç”µè§†"ã€"æ´—è¡£æœº"ã€"æˆ´æ£®"ã€"ç¾çš„"ã€"æ ¼åŠ›"ã€"ç´¢å°¼" â†’ category: "APPLIANCE"
   
   âš ï¸ æ³¨æ„ï¼šcategoryå­—æ®µçš„å€¼å¿…é¡»ä¸æ•°æ®åº“å®é™…å­—æ®µå€¼å®Œå…¨ä¸€è‡´ï¼ŒåŒºåˆ†å¤§å°å†™ï¼ˆå¦‚ELECTRONICSã€BEVERAGEç­‰ï¼‰ï¼Œä¸è¦è¾“å‡ºå°å†™æˆ–å…¶ä»–å˜ä½“ã€‚

2. æŸ¥è¯¢ç±»å‹è¯†åˆ«ï¼š
   - åŒ…å«"é”€å”®"ã€"é”€é‡"ã€"é”€å”®é¢"ã€"å–"ã€"å”®å‡º" â†’ query_type: "é”€å”®åˆ†æ"
   - åŒ…å«"åº“å­˜"ã€"å­˜è´§"ã€"åº“å­˜é‡"ã€"åº“å­˜æƒ…å†µ"ã€"åº“å­˜çŠ¶æ€" â†’ query_type: "åº“å­˜åˆ†æ"
   - åŒ…å«"äº§å“"ã€"å•†å“"ã€"SKU"ã€"äº§å“ä¿¡æ¯"ã€"äº§å“è¯¦æƒ…"ã€"æŸ¥è¯¢"ã€"æŸ¥çœ‹" â†’ query_type: "äº§å“åˆ†æ"
   - åŒ…å«"ä»“åº“"ã€"ä»“"ã€"ä¸­å¿ƒä»“"ã€"ä»“åº“ä¿¡æ¯" â†’ query_type: "ä»“åº“åˆ†æ"
   - åŒ…å«"é—¨åº—"ã€"åº—é“º"ã€"åº—"ã€"é—¨åº—ä¿¡æ¯"ã€"é—¨åº—æƒ…å†µ" â†’ query_type: "é—¨åº—åˆ†æ"
   - åŒ…å«"è¡¥è´§"ã€"è¿›è´§"ã€"é‡‡è´­"ã€"è¡¥è´§æƒ…å†µ" â†’ query_type: "è¡¥è´§åˆ†æ"
   - åŒ…å«"è¶‹åŠ¿"ã€"å˜åŒ–"ã€"å¢é•¿"ã€"è¶‹åŠ¿åˆ†æ" â†’ query_type: "è¶‹åŠ¿åˆ†æ"

3. æ’åºå’Œèšåˆè¯†åˆ«ï¼š
   - åŒ…å«"ä»·æ ¼æœ€é«˜"ã€"æœ€è´µ"ã€"æœ€é«˜ä»·" â†’ order_by: ["unit_price DESC"]
   - åŒ…å«"ä»·æ ¼æœ€ä½"ã€"æœ€ä¾¿å®œ"ã€"æœ€ä½ä»·" â†’ order_by: ["unit_price ASC"]
   - åŒ…å«"é”€é‡æœ€é«˜"ã€"æœ€ç•…é”€"ã€"å–å¾—æœ€å¥½" â†’ order_by: ["total_sales_quantity DESC"]
   - åŒ…å«"é”€å”®é¢æœ€é«˜"ã€"æ”¶å…¥æœ€é«˜"ã€"è¥ä¸šé¢æœ€é«˜" â†’ order_by: ["total_sales_amount DESC"]
   - åŒ…å«"åº“å­˜æœ€å¤š"ã€"åº“å­˜é‡æœ€å¤§" â†’ order_by: ["total_warehouse_stock DESC"]
   - åŒ…å«"åº“å­˜æœ€å°‘"ã€"åº“å­˜ä¸è¶³" â†’ order_by: ["total_warehouse_stock ASC"]

4. ä½ç½®è¯†åˆ«ï¼š
   - åŒ…å«"åŒ—äº¬"ã€"ä¸Šæµ·"ã€"å¹¿å·"ã€"æ·±åœ³"ç­‰åŸå¸‚å â†’ æŸ¥æ‰¾å¯¹åº”çš„é—¨åº—æˆ–ä»“åº“
   - åŒ…å«"ç‹åºœäº•"ã€"å¾å®¶æ±‡"ã€"å¤©æ²³åŸ"ç­‰å…·ä½“åœ°ç‚¹ â†’ æŸ¥æ‰¾å¯¹åº”çš„é—¨åº—
   - åŒ…å«"ååŒ—"ã€"åä¸œ"ã€"åå—"ã€"è¥¿å—"ç­‰åŒºåŸŸ â†’ æŸ¥æ‰¾å¯¹åº”çš„ä»“åº“

5. æ—¶é—´è¯†åˆ«ï¼š
   - åŒ…å«"ä»Šå¤©"ã€"æ˜¨å¤©"ã€"æœ¬å‘¨"ã€"æœ¬æœˆ"ã€"æœ€è¿‘" â†’ è®¾ç½®ç›¸åº”çš„æ—¶é—´èŒƒå›´
   - åŒ…å«"7å¤©"ã€"30å¤©"ã€"ä¸€å‘¨"ã€"ä¸€ä¸ªæœˆ" â†’ è®¾ç½®å…·ä½“çš„æ—¶é—´é—´éš”

6. è¡¨å…³è”è§„åˆ™ï¼š
   - é”€å”®åˆ†æï¼šsales + product + store + warehouse
   - åº“å­˜åˆ†æï¼šwarehouse_inventory + store_inventory + product + warehouse
   - äº§å“åˆ†æï¼šproduct + sales + warehouse_inventory + store_inventory
   - ä»“åº“åˆ†æï¼šwarehouse + warehouse_inventory + replenishment + store
   - é—¨åº—åˆ†æï¼šstore + sales + store_inventory + warehouse
   - è¡¥è´§åˆ†æï¼šreplenishment + warehouse + store + product

7. å­—æ®µæ˜ å°„ï¼š
   - é”€å”®ç›¸å…³ï¼šquantity(æ•°é‡), total_amount(é‡‘é¢), sale_date(é”€å”®æ—¥æœŸ)
   - åº“å­˜ç›¸å…³ï¼šquantity(ä»“åº“åº“å­˜), stock_quantity(é—¨åº—åº“å­˜), safety_stock(å®‰å…¨åº“å­˜)
   - äº§å“ç›¸å…³ï¼šproduct_name(äº§å“å), category(ç±»åˆ«), unit_price(å•ä»·), cost_price(æˆæœ¬ä»·)
   - ä½ç½®ç›¸å…³ï¼šwarehouse_name(ä»“åº“å), store_name(é—¨åº—å), address(åœ°å€)

8. ä¸šåŠ¡æ´å¯Ÿè¯†åˆ«ï¼š
   - "ä»·æ ¼æœ€é«˜" â†’ business_insight: "æŸ¥æ‰¾ä»·æ ¼æœ€é«˜çš„äº§å“ï¼Œä¾¿äºäº†è§£é«˜ç«¯å•†å“å®šä»·"
   - "é”€é‡æœ€å¥½" â†’ business_insight: "åˆ†ææœ€ç•…é”€äº§å“ï¼Œäº†è§£å¸‚åœºéœ€æ±‚"
   - "åº“å­˜ä¸è¶³" â†’ business_insight: "è¯†åˆ«åº“å­˜ä¸è¶³çš„äº§å“ï¼Œéœ€è¦è¡¥è´§"
   - "é”€å”®è¶‹åŠ¿" â†’ business_insight: "åˆ†æäº§å“é”€å”®è¶‹åŠ¿ï¼Œé¢„æµ‹æœªæ¥éœ€æ±‚"

è¯·ä»”ç»†åˆ†æç”¨æˆ·é—®é¢˜ï¼Œå‡†ç¡®è¯†åˆ«æŸ¥è¯¢æ„å›¾ï¼Œç¡®ä¿è¿”å›çš„JSONæ ¼å¼æ­£ç¡®ã€‚
åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
""")
            
            response = self.llm.invoke(intent_prompt.format(
                schema_summary=schema_summary,
                question=question
            ))
            
            # è§£æJSONå“åº”
            intent_data = json.loads(response.content.strip())
            print(f"âœ… æŸ¥è¯¢æ„å›¾åˆ†æå®Œæˆ: {intent_data}")
            
            return intent_data
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢æ„å›¾åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤æ„å›¾
            return {
                "query_type": "ç»¼åˆæŸ¥è¯¢",
                "target_tables": [],
                "target_columns": [],
                "filter_conditions": {},
                "aggregation": {
                    "functions": [],
                    "group_by": [],
                    "order_by": []
                },
                "business_insight": "é€šç”¨æŸ¥è¯¢",
                "confidence": 0.5
            }
    
    def generate_sql_from_intent(self, intent: Dict) -> Optional[str]:
        """åŸºäºæŸ¥è¯¢æ„å›¾ç”ŸæˆSQL"""
        try:
            print(f"ğŸ”§ åŸºäºæ„å›¾ç”ŸæˆSQL: {intent}")
            
            # å¢å¼ºæ„å›¾åˆ†æï¼Œæ·»åŠ ä½ç½®ä¿¡æ¯
            enhanced_intent = self._enhance_intent_with_location(intent)
            print(f"ğŸ”§ å¢å¼ºåçš„æ„å›¾: {enhanced_intent}")
            
            query_type = enhanced_intent.get("query_type", "ç»¼åˆæŸ¥è¯¢")
            target_tables = enhanced_intent.get("target_tables", [])
            filter_conditions = enhanced_intent.get("filter_conditions", {})
            aggregation = enhanced_intent.get("aggregation", {})
            
            # æ ¹æ®æŸ¥è¯¢ç±»å‹ç”Ÿæˆä¸åŒçš„SQL
            if query_type == "é”€å”®åˆ†æ":
                sql = self._generate_sales_sql(enhanced_intent)
            elif query_type == "åº“å­˜åˆ†æ":
                sql = self._generate_inventory_sql(enhanced_intent)
            elif query_type == "äº§å“åˆ†æ":
                sql = self._generate_product_sql(enhanced_intent)
            elif query_type == "ä»“åº“åˆ†æ":
                sql = self._generate_warehouse_sql(enhanced_intent)
            elif query_type == "é—¨åº—åˆ†æ":
                sql = self._generate_store_sql(enhanced_intent)
            elif query_type == "è¡¥è´§åˆ†æ":
                sql = self._generate_replenishment_sql(enhanced_intent)
            elif query_type == "è¶‹åŠ¿åˆ†æ":
                sql = self._generate_trend_sql(enhanced_intent)
            else:
                sql = self._generate_general_sql(enhanced_intent)
            
            print(f"âœ… ç”Ÿæˆçš„SQL: {sql}")
            return sql
            
        except Exception as e:
            print(f"âŒ SQLç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _generate_sales_sql(self, intent: Dict) -> str:
        """ç”Ÿæˆé”€å”®åˆ†æSQL"""
        filter_conditions = intent.get("filter_conditions", {})
        
        sql = """
        SELECT 
            p.product_name,
            p.category,
            st.store_name,
            w.warehouse_name,
            s.sale_date,
            s.quantity,
            s.unit_price,
            s.total_amount
        FROM sales s
        JOIN product p ON s.product_id = p.product_id
        JOIN store st ON s.store_id = st.store_id
        LEFT JOIN warehouse w ON st.warehouse_id = w.warehouse_id
        WHERE 1=1
        """
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filter_conditions.get("category"):
            sql += f" AND p.category = '{filter_conditions['category']}'"
        
        if filter_conditions.get("location"):
            sql += f" AND (st.store_id = '{filter_conditions['location']}' OR w.warehouse_id = '{filter_conditions['location']}')"
        
        if filter_conditions.get("product_name"):
            sql += f" AND p.product_name LIKE '%{filter_conditions['product_name']}%'"
        
        if filter_conditions.get("store_name"):
            sql += f" AND st.store_name LIKE '%{filter_conditions['store_name']}%'"
        
        sql += " ORDER BY s.sale_date DESC LIMIT 20"
        
        return sql
    
    def _generate_inventory_sql(self, intent: Dict) -> str:
        """ç”Ÿæˆåº“å­˜åˆ†æSQL"""
        filter_conditions = intent.get("filter_conditions", {})
        
        sql = """
        SELECT 
            p.product_name,
            p.category,
            w.warehouse_name,
            wi.quantity as warehouse_quantity,
            si.stock_quantity as store_quantity,
            si.safety_stock,
            wi.record_date
        FROM warehouse_inventory wi
        JOIN product p ON wi.product_id = p.product_id
        JOIN warehouse w ON wi.warehouse_id = w.warehouse_id
        LEFT JOIN store_inventory si ON wi.product_id = si.product_id
        WHERE 1=1
        """
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filter_conditions.get("category"):
            sql += f" AND p.category = '{filter_conditions['category']}'"
        
        if filter_conditions.get("location"):
            sql += f" AND (w.warehouse_id = '{filter_conditions['location']}' OR si.store_id = '{filter_conditions['location']}')"
        
        if filter_conditions.get("product_name"):
            sql += f" AND p.product_name LIKE '%{filter_conditions['product_name']}%'"
        
        if filter_conditions.get("warehouse_name"):
            sql += f" AND w.warehouse_name LIKE '%{filter_conditions['warehouse_name']}%'"
        
        sql += " ORDER BY wi.record_date DESC LIMIT 20"
        
        return sql
    
    def _generate_product_sql(self, intent: Dict) -> str:
        """ç”Ÿæˆäº§å“åˆ†æSQL"""
        filter_conditions = intent.get("filter_conditions", {})
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æŒ‰ä»·æ ¼æ’åº
        aggregation = intent.get("aggregation", {})
        order_by = aggregation.get("order_by", [])
        
        # åŸºç¡€SQL
        sql = """
        SELECT 
            p.product_id,
            p.product_name,
            p.category,
            p.unit_price,
            p.cost_price,
            p.barcode,
            COALESCE(SUM(s.quantity), 0) as total_sales_quantity,
            COALESCE(SUM(s.total_amount), 0) as total_sales_amount,
            COALESCE(SUM(wi.quantity), 0) as total_warehouse_stock,
            COALESCE(SUM(si.stock_quantity), 0) as total_store_stock
        FROM product p
        LEFT JOIN sales s ON p.product_id = s.product_id
        LEFT JOIN warehouse_inventory wi ON p.product_id = wi.product_id
        LEFT JOIN store_inventory si ON p.product_id = si.product_id
        WHERE 1=1
        """
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filter_conditions.get("category"):
            sql += f" AND p.category = '{filter_conditions['category']}'"
        
        if filter_conditions.get("product_name"):
            sql += f" AND p.product_name LIKE '%{filter_conditions['product_name']}%'"
        
        sql += " GROUP BY p.product_id, p.product_name, p.category, p.unit_price, p.cost_price, p.barcode"
        
        # æ ¹æ®æ„å›¾é€‰æ‹©æ’åºæ–¹å¼
        if "ä»·æ ¼æœ€é«˜" in intent.get("business_insight", "") or "unit_price DESC" in order_by:
            sql += " ORDER BY p.unit_price DESC"
        elif "ä»·æ ¼æœ€ä½" in intent.get("business_insight", "") or "unit_price ASC" in order_by:
            sql += " ORDER BY p.unit_price ASC"
        elif "é”€é‡æœ€é«˜" in intent.get("business_insight", "") or "total_sales_quantity DESC" in order_by:
            sql += " ORDER BY total_sales_quantity DESC"
        elif "é”€å”®é¢æœ€é«˜" in intent.get("business_insight", "") or "total_sales_amount DESC" in order_by:
            sql += " ORDER BY total_sales_amount DESC"
        else:
            sql += " ORDER BY p.product_name"
        
        sql += " LIMIT 20"
        
        return textwrap.dedent(sql)
    
    def _generate_warehouse_sql(self, intent: Dict) -> str:
        """ç”Ÿæˆä»“åº“åˆ†æSQL"""
        filter_conditions = intent.get("filter_conditions", {})
        
        sql = """
        SELECT 
            w.warehouse_id,
            w.warehouse_name,
            w.address,
            w.created_at,
            COUNT(DISTINCT wi.product_id) as product_count,
            SUM(wi.quantity) as total_inventory,
            COUNT(DISTINCT r.replenishment_id) as replenishment_count,
            COUNT(DISTINCT st.store_id) as store_count
        FROM warehouse w
        LEFT JOIN warehouse_inventory wi ON w.warehouse_id = wi.warehouse_id
        LEFT JOIN replenishment r ON w.warehouse_id = r.warehouse_id
        LEFT JOIN store st ON w.warehouse_id = st.warehouse_id
        WHERE 1=1
        """
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filter_conditions.get("location"):
            sql += f" AND w.warehouse_id = '{filter_conditions['location']}'"
        
        if filter_conditions.get("warehouse_name"):
            sql += f" AND w.warehouse_name LIKE '%{filter_conditions['warehouse_name']}%'"
        
        sql += " GROUP BY w.warehouse_id, w.warehouse_name, w.address, w.created_at"
        sql += " ORDER BY total_inventory DESC LIMIT 20"
        
        return textwrap.dedent(sql)
    
    def _generate_store_sql(self, intent: Dict) -> str:
        """ç”Ÿæˆé—¨åº—åˆ†æSQL"""
        filter_conditions = intent.get("filter_conditions", {})
        
        sql = """
        SELECT 
            st.store_id,
            st.store_name,
            st.address,
            st.opened_date,
            w.warehouse_name,
            COUNT(DISTINCT s.sales_id) as sales_count,
            SUM(s.quantity) as total_sales_quantity,
            SUM(s.total_amount) as total_sales_amount,
            COUNT(DISTINCT si.product_id) as product_count,
            SUM(si.stock_quantity) as total_stock
        FROM store st
        LEFT JOIN warehouse w ON st.warehouse_id = w.warehouse_id
        LEFT JOIN sales s ON st.store_id = s.store_id
        LEFT JOIN store_inventory si ON st.store_id = si.store_id
        WHERE 1=1
        """
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filter_conditions.get("location"):
            sql += f" AND st.store_id = '{filter_conditions['location']}'"
        
        if filter_conditions.get("store_name"):
            sql += f" AND st.store_name LIKE '%{filter_conditions['store_name']}%'"
        
        sql += " GROUP BY st.store_id, st.store_name, st.address, st.opened_date, w.warehouse_name"
        sql += " ORDER BY total_sales_amount DESC LIMIT 20"
        
        return textwrap.dedent(sql)
    
    def _generate_replenishment_sql(self, intent: Dict) -> str:
        """ç”Ÿæˆè¡¥è´§åˆ†æSQL"""
        filter_conditions = intent.get("filter_conditions", {})
        
        sql = """
        SELECT 
            r.replenishment_id,
            w.warehouse_name,
            st.store_name,
            p.product_name,
            p.category,
            r.shipment_date,
            r.shipped_quantity,
            r.received_quantity,
            r.status,
            (r.shipped_quantity - COALESCE(r.received_quantity, 0)) as pending_quantity
        FROM replenishment r
        JOIN warehouse w ON r.warehouse_id = w.warehouse_id
        JOIN store st ON r.store_id = st.store_id
        JOIN product p ON r.product_id = p.product_id
        WHERE 1=1
        """
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filter_conditions.get("location"):
            sql += f" AND (r.warehouse_id = '{filter_conditions['location']}' OR r.store_id = '{filter_conditions['location']}')"
        
        if filter_conditions.get("category"):
            sql += f" AND p.category = '{filter_conditions['category']}'"
        
        sql += " ORDER BY r.shipment_date DESC LIMIT 20"
        
        return textwrap.dedent(sql)
    
    def _generate_trend_sql(self, intent: Dict) -> str:
        """ç”Ÿæˆè¶‹åŠ¿åˆ†æSQL"""
        filter_conditions = intent.get("filter_conditions", {})
        
        sql = """
        SELECT 
            DATE_TRUNC('day', s.sale_date) as sale_day,
            p.category,
            SUM(s.quantity) as daily_sales_quantity,
            SUM(s.total_amount) as daily_sales_amount,
            COUNT(DISTINCT s.sales_id) as daily_order_count
        FROM sales s
        JOIN product p ON s.product_id = p.product_id
        WHERE 1=1
        """
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filter_conditions.get("category"):
            sql += f" AND p.category = '{filter_conditions['category']}'"
        
        if filter_conditions.get("time_range"):
            sql += f" AND s.sale_date >= CURRENT_DATE - INTERVAL '{filter_conditions['time_range']}'"
        
        sql += " GROUP BY DATE_TRUNC('day', s.sale_date), p.category"
        sql += " ORDER BY sale_day DESC, daily_sales_amount DESC LIMIT 20"
        
        return textwrap.dedent(sql)
    
    def _generate_general_sql(self, intent: Dict) -> str:
        """ç”Ÿæˆé€šç”¨æŸ¥è¯¢SQL"""
        # é»˜è®¤æŸ¥è¯¢æ‰€æœ‰è¡¨çš„åŸºæœ¬ä¿¡æ¯
        sql = """
        SELECT 
            'product' as table_name,
            COUNT(*) as record_count,
            'äº§å“ä¿¡æ¯è¡¨' as description
        FROM product
        UNION ALL
        SELECT 
            'sales' as table_name,
            COUNT(*) as record_count,
            'é”€å”®è®°å½•è¡¨' as description
        FROM sales
        UNION ALL
        SELECT 
            'warehouse' as table_name,
            COUNT(*) as record_count,
            'ä»“åº“ä¿¡æ¯è¡¨' as description
        FROM warehouse
        UNION ALL
        SELECT 
            'store' as table_name,
            COUNT(*) as record_count,
            'é—¨åº—ä¿¡æ¯è¡¨' as description
        FROM store
        ORDER BY record_count DESC
        """
        
        return sql
    
    def _map_location_name(self, location_name: str) -> Dict:
        """æ™ºèƒ½æ˜ å°„ä½ç½®åç§°åˆ°æ•°æ®åº“ID"""
        try:
            # åŸå¸‚åˆ°é—¨åº—/ä»“åº“çš„æ˜ å°„
            city_mapping = {
                "åŒ—äº¬": {"stores": ["ST101"], "warehouses": ["WH001"]},
                "ä¸Šæµ·": {"stores": ["ST102"], "warehouses": ["WH002"]},
                "å¹¿å·": {"stores": ["ST103"], "warehouses": ["WH003"]},
                "æ·±åœ³": {"stores": ["ST104"], "warehouses": ["WH003"]},
                "æˆéƒ½": {"stores": ["ST105"], "warehouses": ["WH004"]},
                "é‡åº†": {"stores": ["ST106"], "warehouses": ["WH004"]},
                "æ­¦æ±‰": {"stores": ["ST107"], "warehouses": ["WH004"]},
                "å—äº¬": {"stores": ["ST108"], "warehouses": ["WH002"]},
                "æ­å·": {"stores": ["ST109"], "warehouses": ["WH002"]},
                "è¥¿å®‰": {"stores": ["ST110"], "warehouses": ["WH001"]}
            }
            
            # å…·ä½“åœ°ç‚¹åˆ°é—¨åº—çš„æ˜ å°„
            place_mapping = {
                "ç‹åºœäº•": "ST101",
                "å¾å®¶æ±‡": "ST102", 
                "å¤©æ²³åŸ": "ST103",
                "ä¸‡è±¡åŸ": "ST104",
                "æ˜¥ç†™è·¯": "ST105",
                "è§£æ”¾ç¢‘": "ST106",
                "æ­¦å•†å¹¿åœº": "ST107",
                "æ–°è¡—å£": "ST108",
                "è¥¿æ¹–": "ST109",
                "é’Ÿæ¥¼": "ST110"
            }
            
            # åŒºåŸŸåˆ°ä»“åº“çš„æ˜ å°„
            region_mapping = {
                "ååŒ—": "WH001",
                "åä¸œ": "WH002",
                "åå—": "WH003", 
                "è¥¿å—": "WH004",
                "ä¸œåŒ—": "WH005"
            }
            
            # æ£€æŸ¥åŸå¸‚æ˜ å°„
            for city, mapping in city_mapping.items():
                if city in location_name:
                    return mapping
            
            # æ£€æŸ¥å…·ä½“åœ°ç‚¹æ˜ å°„
            for place, store_id in place_mapping.items():
                if place in location_name:
                    return {"stores": [store_id], "warehouses": []}
            
            # æ£€æŸ¥åŒºåŸŸæ˜ å°„
            for region, warehouse_id in region_mapping.items():
                if region in location_name:
                    return {"stores": [], "warehouses": [warehouse_id]}
            
            return {"stores": [], "warehouses": []}
            
        except Exception as e:
            print(f"âš ï¸ ä½ç½®æ˜ å°„å¤±è´¥: {e}")
            return {"stores": [], "warehouses": []}
    
    def _enhance_intent_with_location(self, intent: Dict) -> Dict:
        """å¢å¼ºæ„å›¾åˆ†æï¼Œæ·»åŠ ä½ç½®ä¿¡æ¯"""
        try:
            filter_conditions = intent.get("filter_conditions", {})
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä½ç½®ç›¸å…³çš„è¿‡æ»¤æ¡ä»¶
            if filter_conditions.get("location"):
                location_name = filter_conditions["location"]
                location_mapping = self._map_location_name(location_name)
                
                # æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©åˆé€‚çš„ID
                query_type = intent.get("query_type", "")
                
                if query_type in ["é—¨åº—åˆ†æ", "é”€å”®åˆ†æ"] and location_mapping["stores"]:
                    filter_conditions["location"] = location_mapping["stores"][0]
                elif query_type in ["ä»“åº“åˆ†æ", "åº“å­˜åˆ†æ"] and location_mapping["warehouses"]:
                    filter_conditions["location"] = location_mapping["warehouses"][0]
                elif location_mapping["stores"]:
                    filter_conditions["location"] = location_mapping["stores"][0]
                elif location_mapping["warehouses"]:
                    filter_conditions["location"] = location_mapping["warehouses"][0]
            
            intent["filter_conditions"] = filter_conditions
            return intent
            
        except Exception as e:
            print(f"âš ï¸ æ„å›¾ä½ç½®å¢å¼ºå¤±è´¥: {e}")
            return intent

    def generate_sql(self, question: str) -> Optional[str]:
        """ä½¿ç”¨LLMç”ŸæˆSQLæŸ¥è¯¢"""
        try:
            # 1. é¦–å…ˆè¿›è¡ŒæŸ¥è¯¢æ„å›¾åˆ†æ
            intent = self.analyze_query_intent(question)
            
            # 2. åŸºäºæ„å›¾ç”ŸæˆSQL
            sql = self.generate_sql_from_intent(intent)
            
            if sql and sql.upper().startswith('SELECT'):
                return sql
            return None
            
        except Exception as e:
            print(f"âŒ SQLç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def execute_query(self, sql: str) -> List[Tuple]:
        try:
            print(f"ğŸš€ æ‰§è¡ŒSQLæŸ¥è¯¢: {repr(sql)}")
            print(f"SQLç±»å‹: {type(sql)}")
            assert self.conn and self.conn.closed == 0, "æ•°æ®åº“è¿æ¥å·²å…³é—­"
            cursor = self.conn.cursor()
            cursor.execute(sql)
            rows = cursor.fetchall()
            cursor.close()
            print(f"âœ… æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(rows)} è¡Œæ•°æ®")
            return rows
        except Exception as e:
            print(f"âŒ SQLæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # å†™å…¥æ—¥å¿—
            with open("sql_error.log", "a", encoding="utf-8") as f:
                f.write(f"SQLæ‰§è¡Œå¤±è´¥: {repr(sql)}\né”™è¯¯: {e}\n")
            return []
    
    def get_column_names(self, sql: str) -> List[str]:
        """è·å–æŸ¥è¯¢ç»“æœçš„åˆ—å"""
        try:
            cursor = self.conn.cursor()
            cursor.execute(sql)
            column_names = [desc[0] for desc in cursor.description]
            cursor.close()
            return column_names
        except Exception as e:
            print(f"âŒ è·å–åˆ—åå¤±è´¥: {e}")
            return []
    
    def analyze_data_statistics(self, rows: List[Tuple], column_names: List[str]) -> Dict:
        """åˆ†ææ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if not rows or not column_names:
            return {}
        
        stats = {}
        try:
            # è½¬æ¢ä¸ºDataFrameæ ¼å¼è¿›è¡Œåˆ†æ
            data_dict = {}
            for i, col_name in enumerate(column_names):
                data_dict[col_name] = [row[i] for row in rows]
            
            # æ•°å€¼å‹åˆ—ç»Ÿè®¡
            numeric_stats = {}
            for col_name, values in data_dict.items():
                try:
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                    numeric_values = []
                    for val in values:
                        if val is not None:
                            try:
                                numeric_values.append(float(val))
                            except (ValueError, TypeError):
                                continue
                    
                    if numeric_values:
                        numeric_stats[col_name] = {
                            'count': len(numeric_values),
                            'min': min(numeric_values),
                            'max': max(numeric_values),
                            'avg': sum(numeric_values) / len(numeric_values),
                            'sum': sum(numeric_values)
                        }
                except Exception:
                    continue
            
            # åˆ†ç±»åˆ—ç»Ÿè®¡
            categorical_stats = {}
            for col_name, values in data_dict.items():
                if col_name not in numeric_stats:
                    try:
                        value_counts = {}
                        for val in values:
                            if val is not None:
                                val_str = str(val)
                                value_counts[val_str] = value_counts.get(val_str, 0) + 1
                        
                        if value_counts:
                            categorical_stats[col_name] = {
                                'unique_count': len(value_counts),
                                'top_values': sorted(value_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                            }
                    except Exception:
                        continue
            
            stats = {
                'total_rows': len(rows),
                'numeric_columns': numeric_stats,
                'categorical_columns': categorical_stats
            }
            
        except Exception as e:
            print(f"âš ï¸ æ•°æ®ç»Ÿè®¡åˆ†æå¤±è´¥: {e}")
        
        return stats
    
    def analyze_data_trends(self, rows: List[Tuple], column_names: List[str]) -> Dict:
        """åˆ†ææ•°æ®è¶‹åŠ¿"""
        if not rows or not column_names:
            return {}
        
        trends = {}
        try:
            # æŸ¥æ‰¾æ—¶é—´ç›¸å…³åˆ—
            time_columns = []
            for col_name in column_names:
                if any(keyword in col_name.lower() for keyword in ['time', 'date', 'created', 'updated', 'timestamp']):
                    time_columns.append(col_name)
            
            if time_columns:
                # åˆ†ææ—¶é—´è¶‹åŠ¿
                for time_col in time_columns:
                    try:
                        time_idx = column_names.index(time_col)
                        time_values = [row[time_idx] for row in rows if row[time_idx] is not None]
                        
                        if time_values:
                            # ç®€å•çš„æ—¶é—´è¶‹åŠ¿åˆ†æ
                            trends[time_col] = {
                                'earliest': min(time_values),
                                'latest': max(time_values),
                                'total_periods': len(time_values)
                            }
                    except Exception:
                        continue
            
            # åˆ†ææ•°å€¼è¶‹åŠ¿
            data_dict = {}
            for i, col_name in enumerate(column_names):
                data_dict[col_name] = [row[i] for row in rows]
            
            for col_name, values in data_dict.items():
                try:
                    numeric_values = []
                    for val in values:
                        if val is not None:
                            try:
                                numeric_values.append(float(val))
                            except (ValueError, TypeError):
                                continue
                    
                    if len(numeric_values) > 1:
                        # è®¡ç®—è¶‹åŠ¿ï¼ˆç®€å•çº¿æ€§è¶‹åŠ¿ï¼‰
                        sorted_values = sorted(numeric_values)
                        if sorted_values[0] != sorted_values[-1]:
                            trend_direction = "ä¸Šå‡" if sorted_values[-1] > sorted_values[0] else "ä¸‹é™"
                            trends[f"{col_name}_trend"] = {
                                'direction': trend_direction,
                                'range': f"{sorted_values[0]} - {sorted_values[-1]}",
                                'variation': sorted_values[-1] - sorted_values[0]
                            }
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
        
        return trends
    
    def analyze_data_relationships(self, rows: List[Tuple], column_names: List[str]) -> Dict:
        """åˆ†ææ•°æ®å…³è”å…³ç³»"""
        if not rows or not column_names:
            return {}
        
        relationships = {}
        try:
            # åˆ†æå¤–é”®å…³ç³»
            for table_name, rels in self.schema_analyzer.table_relationships.items():
                for rel in rels:
                    relationships[f"{table_name}.{rel['column']}"] = {
                        'references': f"{rel['foreign_table']}.{rel['foreign_column']}",
                        'type': 'foreign_key'
                    }
            
            # åˆ†ææ•°æ®ä¸­çš„å…³è”æ¨¡å¼
            data_dict = {}
            for i, col_name in enumerate(column_names):
                data_dict[col_name] = [row[i] for row in rows]
            
            # æŸ¥æ‰¾å¯èƒ½çš„å…³è”åˆ—ï¼ˆç›¸åŒå€¼çš„åˆ—ï¼‰
            for col1 in column_names:
                for col2 in column_names:
                    if col1 != col2:
                        try:
                            values1 = set(str(data_dict[col1][i]) for i in range(len(rows)) if data_dict[col1][i] is not None)
                            values2 = set(str(data_dict[col2][i]) for i in range(len(rows)) if data_dict[col2][i] is not None)
                            
                            # è®¡ç®—é‡å åº¦
                            overlap = len(values1.intersection(values2))
                            if overlap > 0 and len(values1) > 0 and len(values2) > 0:
                                overlap_ratio = overlap / min(len(values1), len(values2))
                                if overlap_ratio > 0.3:  # 30%ä»¥ä¸Šé‡å è®¤ä¸ºæœ‰å…³è”
                                    relationships[f"{col1}_vs_{col2}"] = {
                                        'overlap_count': overlap,
                                        'overlap_ratio': overlap_ratio,
                                        'type': 'data_overlap'
                                    }
                        except Exception:
                            continue
        
        except Exception as e:
            print(f"âš ï¸ å…³è”å…³ç³»åˆ†æå¤±è´¥: {e}")
        
        return relationships
    

    
    def query(self, question: str, context: str = "") -> str:
        """é€šç”¨æ•°æ®åº“æŸ¥è¯¢æ¥å£ - ç›´æ¥æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›å…·ä½“æ•°æ®"""
        try:
            print(f"ğŸ” å¼€å§‹å¤„ç†æŸ¥è¯¢: {question}")
            
            # 1. æ™ºèƒ½ç”ŸæˆSQLæŸ¥è¯¢
            sql = self._generate_intelligent_sql(question)
            
            if not sql:
                print("âŒ SQLç”Ÿæˆå¤±è´¥")
                return "æ— æ³•ç†è§£æŸ¥è¯¢éœ€æ±‚ï¼Œè¯·æä¾›æ›´å…·ä½“çš„é—®é¢˜"
            
            print(f"âœ… ç”Ÿæˆçš„SQL: {sql}")
            
            # 2. æ‰§è¡ŒæŸ¥è¯¢
            rows = self.execute_query(sql)
            
            if not rows:
                print("âŒ æŸ¥è¯¢è¿”å›ç©ºç»“æœ")
                return "æœªæ‰¾åˆ°ç›¸å…³æ•°æ®ï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢æ¡ä»¶"
            
            print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(rows)} æ¡è®°å½•")
            
            # 3. è·å–åˆ—å
            column_names = self.get_column_names(sql)
            if not column_names:
                column_names = [f"column_{i}" for i in range(len(rows[0]) if rows else 0)]
            
            print(f"âœ… åˆ—å: {column_names}")
            
            # 4. ç›´æ¥è¿”å›å…·ä½“æ•°æ®å’Œåˆ†æ
            result = self._format_comprehensive_results(question, rows, column_names, sql)
            print(f"âœ… ç»“æœæ ¼å¼åŒ–å®Œæˆï¼Œé•¿åº¦: {len(result)} å­—ç¬¦")
            
            return result
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}"
    
    def _generate_intelligent_sql(self, question: str) -> Optional[str]:
        """æ™ºèƒ½ç”ŸæˆSQLæŸ¥è¯¢ - ä½¿ç”¨æ„å›¾åˆ†æ"""
        try:
            print(f"ğŸ§  å¼€å§‹æ™ºèƒ½SQLç”Ÿæˆï¼Œé—®é¢˜: {question}")
            
            # ä½¿ç”¨æ–°çš„æ„å›¾åˆ†æåŠŸèƒ½
            return self.generate_sql(question)
            
        except Exception as e:
            print(f"âŒ æ™ºèƒ½SQLç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def execute_query_with_columns(self, sql: str):
        """æ‰§è¡ŒSQLå¹¶è¿”å› (rows, column_names)"""
        try:
            print(f"ğŸš€ æ‰§è¡ŒSQLæŸ¥è¯¢: {repr(sql)}")
            print(f"SQLç±»å‹: {type(sql)}")
            assert self.conn and self.conn.closed == 0, "æ•°æ®åº“è¿æ¥å·²å…³é—­"
            cursor = self.conn.cursor()
            cursor.execute(sql)
            rows = cursor.fetchall()
            column_names = [desc[0] for desc in cursor.description]
            cursor.close()
            print(f"âœ… æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(rows)} è¡Œæ•°æ®")
            return rows, column_names
        except Exception as e:
            print(f"âŒ SQLæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            with open("sql_error.log", "a", encoding="utf-8") as f:
                f.write(f"SQLæ‰§è¡Œå¤±è´¥: {repr(sql)}\\né”™è¯¯: {e}\\n")
            return [], []
    
    def get_column_names(self, sql: str) -> List[str]:
        """è·å–æŸ¥è¯¢ç»“æœçš„åˆ—å"""
        try:
            print(f"ğŸ“‹ è·å–åˆ—åï¼ŒSQL: {sql}")
            cursor = self.conn.cursor()
            cursor.execute(sql)
            column_names = [desc[0] for desc in cursor.description]
            cursor.close()
            print(f"âœ… è·å–åˆ—åæˆåŠŸ: {column_names}")
            return column_names
        except Exception as e:
            print(f"âŒ è·å–åˆ—åå¤±è´¥: {e}")
            return []
    
    def analyze_data_statistics(self, rows: List[Tuple], column_names: List[str]) -> Dict:
        """åˆ†ææ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if not rows or not column_names:
            return {}
        
        stats = {}
        try:
            # è½¬æ¢ä¸ºDataFrameæ ¼å¼è¿›è¡Œåˆ†æ
            data_dict = {}
            for i, col_name in enumerate(column_names):
                data_dict[col_name] = [row[i] for row in rows]
            
            # æ•°å€¼å‹åˆ—ç»Ÿè®¡
            numeric_stats = {}
            for col_name, values in data_dict.items():
                try:
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                    numeric_values = []
                    for val in values:
                        if val is not None:
                            try:
                                numeric_values.append(float(val))
                            except (ValueError, TypeError):
                                continue
                    
                    if numeric_values:
                        numeric_stats[col_name] = {
                            'count': len(numeric_values),
                            'sum': sum(values),
                            'avg': sum(values) / len(values),
                            'min': min(values),
                            'max': max(values)
                        }
                except Exception:
                    continue
            
            # åˆ†ç±»åˆ—ç»Ÿè®¡
            categorical_stats = {}
            for col_name, values in data_dict.items():
                if col_name not in numeric_stats:
                    try:
                        value_counts = {}
                        for val in values:
                            if val is not None:
                                val_str = str(val)
                                value_counts[val_str] = value_counts.get(val_str, 0) + 1
                        
                        if value_counts:
                            categorical_stats[col_name] = {
                                'unique_count': len(value_counts),
                                'top_values': sorted(value_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                            }
                    except Exception:
                        continue
            
            stats = {
                'total_rows': len(rows),
                'numeric_columns': numeric_stats,
                'categorical_columns': categorical_stats
            }
            
        except Exception as e:
            print(f"âš ï¸ æ•°æ®ç»Ÿè®¡åˆ†æå¤±è´¥: {e}")
        
        return stats
    
    def analyze_data_trends(self, rows: List[Tuple], column_names: List[str]) -> Dict:
        """åˆ†ææ•°æ®è¶‹åŠ¿"""
        if not rows or not column_names:
            return {}
        
        trends = {}
        try:
            # æŸ¥æ‰¾æ—¶é—´ç›¸å…³åˆ—
            time_columns = []
            for col_name in column_names:
                if any(keyword in col_name.lower() for keyword in ['time', 'date', 'created', 'updated', 'timestamp']):
                    time_columns.append(col_name)
            
            if time_columns:
                # åˆ†ææ—¶é—´è¶‹åŠ¿
                for time_col in time_columns:
                    try:
                        time_idx = column_names.index(time_col)
                        time_values = [row[time_idx] for row in rows if row[time_idx] is not None]
                        
                        if time_values:
                            # ç®€å•çš„æ—¶é—´è¶‹åŠ¿åˆ†æ
                            trends[time_col] = {
                                'earliest': min(time_values),
                                'latest': max(time_values),
                                'total_periods': len(time_values)
                            }
                    except Exception:
                        continue
            
            # åˆ†ææ•°å€¼è¶‹åŠ¿
            data_dict = {}
            for i, col_name in enumerate(column_names):
                data_dict[col_name] = [row[i] for row in rows]
            
            for col_name, values in data_dict.items():
                try:
                    numeric_values = []
                    for val in values:
                        if val is not None:
                            try:
                                numeric_values.append(float(val))
                            except (ValueError, TypeError):
                                continue
                    
                    if len(numeric_values) > 1:
                        # è®¡ç®—è¶‹åŠ¿ï¼ˆç®€å•çº¿æ€§è¶‹åŠ¿ï¼‰
                        sorted_values = sorted(numeric_values)
                        if sorted_values[0] != sorted_values[-1]:
                            trend_direction = "ä¸Šå‡" if sorted_values[-1] > sorted_values[0] else "ä¸‹é™"
                            trends[f"{col_name}_trend"] = {
                                'direction': trend_direction,
                                'range': f"{sorted_values[0]} - {sorted_values[-1]}",
                                'variation': sorted_values[-1] - sorted_values[0]
                            }
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
        
        return trends
    
    def analyze_data_relationships(self, rows: List[Tuple], column_names: List[str]) -> Dict:
        """åˆ†ææ•°æ®å…³è”å…³ç³»"""
        if not rows or not column_names:
            return {}
        
        relationships = {}
        try:
            # åˆ†æå¤–é”®å…³ç³»
            for table_name, rels in self.schema_analyzer.table_relationships.items():
                for rel in rels:
                    relationships[f"{table_name}.{rel['column']}"] = {
                        'references': f"{rel['foreign_table']}.{rel['foreign_column']}",
                        'type': 'foreign_key'
                    }
            
            # åˆ†ææ•°æ®ä¸­çš„å…³è”æ¨¡å¼
            data_dict = {}
            for i, col_name in enumerate(column_names):
                data_dict[col_name] = [row[i] for row in rows]
            
            # æŸ¥æ‰¾å¯èƒ½çš„å…³è”åˆ—ï¼ˆç›¸åŒå€¼çš„åˆ—ï¼‰
            for col1 in column_names:
                for col2 in column_names:
                    if col1 != col2:
                        try:
                            values1 = set(str(data_dict[col1][i]) for i in range(len(rows)) if data_dict[col1][i] is not None)
                            values2 = set(str(data_dict[col2][i]) for i in range(len(rows)) if data_dict[col2][i] is not None)
                            
                            # è®¡ç®—é‡å åº¦
                            overlap = len(values1.intersection(values2))
                            if overlap > 0 and len(values1) > 0 and len(values2) > 0:
                                overlap_ratio = overlap / min(len(values1), len(values2))
                                if overlap_ratio > 0.3:  # 30%ä»¥ä¸Šé‡å è®¤ä¸ºæœ‰å…³è”
                                    relationships[f"{col1}_vs_{col2}"] = {
                                        'overlap_count': overlap,
                                        'overlap_ratio': overlap_ratio,
                                        'type': 'data_overlap'
                                    }
                        except Exception:
                            continue
        
        except Exception as e:
            print(f"âš ï¸ å…³è”å…³ç³»åˆ†æå¤±è´¥: {e}")
        
        return relationships
    

    
    def _format_comprehensive_results(self, question: str, rows: List[Tuple], column_names: List[str], sql: str) -> str:
        """ç»¼åˆæ ¼å¼åŒ–æŸ¥è¯¢ç»“æœï¼Œè¿”å›å…·ä½“æ•°æ®"""
        try:
            result = f"ğŸ“Š æŸ¥è¯¢ç»“æœï¼šå…±æ‰¾åˆ° {len(rows)} æ¡è®°å½•\n\n"
            
            # 1. æ˜¾ç¤ºè¡¨å¤´
            result += "ğŸ“‹ æ•°æ®æ˜ç»†ï¼š\n"
            result += " | ".join(f"{name:<15}" for name in column_names) + "\n"
            result += "-" * (len(column_names) * 18) + "\n"
            
            # 2. æ˜¾ç¤ºæ•°æ®ï¼ˆæœ€å¤šæ˜¾ç¤º15è¡Œï¼‰
            for i, row in enumerate(rows[:15]):
                formatted_row = []
                for value in row:
                    if value is None:
                        formatted_row.append("NULL".ljust(15))
                    else:
                        str_value = str(value)
                        if len(str_value) > 15:
                            str_value = str_value[:12] + "..."
                        formatted_row.append(str_value.ljust(15))
                result += " | ".join(formatted_row) + "\n"
            
            if len(rows) > 15:
                result += f"... è¿˜æœ‰ {len(rows) - 15} æ¡è®°å½•\n"
            
            # 3. æ·»åŠ ç»Ÿè®¡åˆ†æ
            result += "\nğŸ“ˆ ç»Ÿè®¡åˆ†æï¼š\n"
            
            # æ•°å€¼åˆ—ç»Ÿè®¡
            numeric_stats = self._calculate_numeric_stats(rows, column_names)
            if numeric_stats:
                result += "æ•°å€¼ç»Ÿè®¡ï¼š\n"
                for col, stats in numeric_stats.items():
                    result += f"  {col}: æ€»è®¡{stats['sum']:,.2f}, å¹³å‡{stats['avg']:.2f}, èŒƒå›´{stats['min']}-{stats['max']}\n"
            
            # åˆ†ç±»ç»Ÿè®¡
            categorical_stats = self._calculate_categorical_stats(rows, column_names)
            if categorical_stats:
                result += "\nåˆ†ç±»ç»Ÿè®¡ï¼š\n"
                for col, stats in categorical_stats.items():
                    result += f"  {col}: {stats['unique_count']}ä¸ªä¸åŒå€¼\n"
                    if stats['top_values']:
                        top_val = stats['top_values'][0]
                        result += f"    æœ€å¸¸è§: {top_val[0]} ({top_val[1]}æ¬¡)\n"
            
            # 4. ä¸šåŠ¡æ´å¯Ÿ
            result += "\nğŸ’¡ ä¸šåŠ¡æ´å¯Ÿï¼š\n"
            insight = self._generate_comprehensive_insight(question, rows, column_names, sql)
            result += insight
            
            # 5. æ•°æ®æ‘˜è¦
            result += "\nğŸ“‹ æ•°æ®æ‘˜è¦ï¼š\n"
            result += f"â€¢ æŸ¥è¯¢å­—æ®µï¼š{', '.join(column_names)}\n"
            result += f"â€¢ æ•°æ®æ—¶é—´ï¼šæœ€æ–°è®°å½•åŒ…å«{len(rows)}æ¡æ•°æ®\n"
            result += f"â€¢ æŸ¥è¯¢ç±»å‹ï¼š{self._identify_query_type(question)}\n"
            
            return result
            
        except Exception as e:
            return f"ç»“æœæ ¼å¼åŒ–å¤±è´¥: {str(e)}"
    
    def _calculate_numeric_stats(self, rows: List[Tuple], column_names: List[str]) -> Dict:
        """è®¡ç®—æ•°å€¼åˆ—ç»Ÿè®¡"""
        stats = {}
        for i, col_name in enumerate(column_names):
            try:
                values = []
                for row in rows:
                    if row[i] is not None:
                        try:
                            values.append(float(row[i]))
                        except (ValueError, TypeError):
                            continue
                
                if values:
                    stats[col_name] = {
                        'count': len(values),
                        'sum': sum(values),
                        'avg': sum(values) / len(values),
                        'min': min(values),
                        'max': max(values)
                    }
            except Exception:
                continue
        return stats
    
    def _calculate_categorical_stats(self, rows: List[Tuple], column_names: List[str]) -> Dict:
        """è®¡ç®—åˆ†ç±»åˆ—ç»Ÿè®¡"""
        stats = {}
        for i, col_name in enumerate(column_names):
            try:
                value_counts = {}
                for row in rows:
                    if row[i] is not None:
                        val_str = str(row[i])
                        value_counts[val_str] = value_counts.get(val_str, 0) + 1
                
                if value_counts:
                    stats[col_name] = {
                        'unique_count': len(value_counts),
                        'top_values': sorted(value_counts.items(), key=lambda x: x[1], reverse=True)[:3]
                    }
            except Exception:
                continue
        return stats
    
    def _identify_query_type(self, question: str) -> str:
        """è¯†åˆ«æŸ¥è¯¢ç±»å‹"""
        if any(keyword in question for keyword in ["é”€å”®", "é”€å”®é¢", "é”€å”®æƒ…å†µ"]):
            return "é”€å”®åˆ†æ"
        elif any(keyword in question for keyword in ["åº“å­˜", "å­˜è´§", "åº“å­˜æƒ…å†µ"]):
            return "åº“å­˜åˆ†æ"
        elif any(keyword in question for keyword in ["äº§å“", "å•†å“", "SKU"]):
            return "äº§å“åˆ†æ"
        elif any(keyword in question for keyword in ["ä»“åº“", "ä»“", "ä¸­å¿ƒä»“"]):
            return "ä»“åº“åˆ†æ"
        elif any(keyword in question for keyword in ["è¶‹åŠ¿", "å˜åŒ–", "å¢é•¿"]):
            return "è¶‹åŠ¿åˆ†æ"
        else:
            return "é€šç”¨æŸ¥è¯¢"
    
    def _generate_comprehensive_insight(self, question: str, rows: List[Tuple], column_names: List[str], sql: str) -> str:
        """ç”Ÿæˆç»¼åˆä¸šåŠ¡æ´å¯Ÿï¼ˆåªè¾“å‡ºä¸€æ¬¡ï¼ŒæŒ‰query_typeåˆ†æµï¼‰"""
        try:
            insight = ""
            query_type = self._identify_query_type(question)
            if query_type == "é”€å”®åˆ†æ":
                insight += self._generate_sales_insight(rows, column_names)
            elif query_type == "åº“å­˜åˆ†æ":
                insight += self._generate_inventory_insight(rows, column_names)
            elif query_type == "äº§å“åˆ†æ":
                insight += self._generate_product_insight(rows, column_names)
            elif query_type == "ä»“åº“åˆ†æ":
                insight += self._generate_warehouse_insight(rows, column_names)
            elif query_type == "è¶‹åŠ¿åˆ†æ":
                insight += self._generate_trend_insight(rows, column_names)
            else:
                insight += self._generate_general_insight(rows, column_names, f"æŸ¥è¯¢è¿”å›{len(rows)}æ¡è®°å½•")
            return insight
        except Exception as e:
            return f"ä¸šåŠ¡æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _generate_product_insight(self, rows: List[Tuple], column_names: List[str]) -> str:
        """ç”Ÿæˆäº§å“æ´å¯Ÿ"""
        try:
            insight = ""
            
            # æŸ¥æ‰¾äº§å“ç›¸å…³å­—æ®µ
            product_name_idx = -1
            category_idx = -1
            price_idx = -1
            sales_idx = -1
            
            for i, col in enumerate(column_names):
                if 'product_name' in col.lower() or 'name' in col.lower():
                    product_name_idx = i
                elif 'category' in col.lower():
                    category_idx = i
                elif 'price' in col.lower():
                    price_idx = i
                elif 'sales' in col.lower() or 'amount' in col.lower():
                    sales_idx = i
            
            if product_name_idx >= 0:
                products = set()
                for row in rows:
                    if row[product_name_idx] is not None:
                        products.add(str(row[product_name_idx]))
                insight += f"â€¢ æ¶‰åŠäº§å“ï¼š{len(products)}ç§\n"
                
                if len(products) <= 5:
                    insight += f"â€¢ äº§å“åˆ—è¡¨ï¼š{', '.join(list(products)[:5])}\n"
            
            if category_idx >= 0:
                categories = {}
                for row in rows:
                    if row[category_idx] is not None:
                        cat = str(row[category_idx])
                        categories[cat] = categories.get(cat, 0) + 1
                
                if categories:
                    top_category = max(categories.items(), key=lambda x: x[1])
                    insight += f"â€¢ ä¸»è¦ç±»åˆ«ï¼š{top_category[0]} ({top_category[1]}æ¡è®°å½•)\n"
            
            if sales_idx >= 0:
                total_sales = sum(float(row[sales_idx]) for row in rows if row[sales_idx] is not None)
                insight += f"â€¢ æ€»é”€å”®é¢ï¼šÂ¥{total_sales:,.2f}\n"
            
            return insight
            
        except Exception as e:
            return f"äº§å“æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _generate_warehouse_insight(self, rows: List[Tuple], column_names: List[str]) -> str:
        """ç”Ÿæˆä»“åº“æ´å¯Ÿ"""
        try:
            insight = ""
            
            # æŸ¥æ‰¾ä»“åº“ç›¸å…³å­—æ®µ
            warehouse_name_idx = -1
            inventory_idx = -1
            
            for i, col in enumerate(column_names):
                if 'warehouse' in col.lower():
                    warehouse_name_idx = i
                elif 'inventory' in col.lower() or 'stock' in col.lower():
                    inventory_idx = i
            
            if warehouse_name_idx >= 0:
                warehouses = set()
                for row in rows:
                    if row[warehouse_name_idx] is not None:
                        warehouses.add(str(row[warehouse_name_idx]))
                insight += f"â€¢ æ¶‰åŠä»“åº“ï¼š{len(warehouses)}ä¸ª\n"
                
                if len(warehouses) <= 5:
                    insight += f"â€¢ ä»“åº“åˆ—è¡¨ï¼š{', '.join(list(warehouses)[:5])}\n"
            
            if inventory_idx >= 0:
                total_inventory = sum(float(row[inventory_idx]) for row in rows if row[inventory_idx] is not None)
                insight += f"â€¢ æ€»åº“å­˜é‡ï¼š{total_inventory:,.0f}\n"
            
            return insight
            
        except Exception as e:
            return f"ä»“åº“æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _generate_sales_insight(self, rows: List[Tuple], column_names: List[str]) -> str:
        """ç”Ÿæˆé”€å”®æ´å¯Ÿ"""
        try:
            insight = ""
            
            # è®¡ç®—æ€»é”€å”®é¢
            total_sales = 0
            total_quantity = 0
            product_sales = {}
            warehouse_sales = {}
            
            for row in rows:
                amount_idx = column_names.index('total_amount') if 'total_amount' in column_names else -1
                quantity_idx = column_names.index('total_quantity') if 'total_quantity' in column_names else -1
                product_idx = column_names.index('product_name') if 'product_name' in column_names else -1
                warehouse_idx = column_names.index('warehouse_name') if 'warehouse_name' in column_names else -1
                
                if amount_idx >= 0 and row[amount_idx] is not None:
                    total_sales += float(row[amount_idx])
                
                if quantity_idx >= 0 and row[quantity_idx] is not None:
                    total_quantity += float(row[quantity_idx])
                
                if product_idx >= 0 and amount_idx >= 0:
                    product = str(row[product_idx])
                    amount = float(row[amount_idx]) if row[amount_idx] is not None else 0
                    product_sales[product] = product_sales.get(product, 0) + amount
                
                if warehouse_idx >= 0 and amount_idx >= 0:
                    warehouse = str(row[warehouse_idx])
                    amount = float(row[amount_idx]) if row[amount_idx] is not None else 0
                    warehouse_sales[warehouse] = warehouse_sales.get(warehouse, 0) + amount
            
            insight += f"â€¢ æ€»é”€å”®é¢ï¼šÂ¥{total_sales:,.2f}\n"
            insight += f"â€¢ æ€»é”€å”®æ•°é‡ï¼š{total_quantity:,.0f}\n"
            
            if product_sales:
                top_product = max(product_sales.items(), key=lambda x: x[1])
                insight += f"â€¢ çƒ­é”€äº§å“ï¼š{top_product[0]} (Â¥{top_product[1]:,.2f})\n"
            
            if warehouse_sales:
                top_warehouse = max(warehouse_sales.items(), key=lambda x: x[1])
                insight += f"â€¢ é”€å”®æœ€ä½³ä»“åº“ï¼š{top_warehouse[0]} (Â¥{top_warehouse[1]:,.2f})\n"
            
            return insight
            
        except Exception as e:
            return f"é”€å”®æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _generate_inventory_insight(self, rows: List[Tuple], column_names: List[str]) -> str:
        """ç”Ÿæˆåº“å­˜æ´å¯Ÿ"""
        try:
            insight = ""
            
            total_value = 0
            low_stock_count = 0
            normal_stock_count = 0
            high_stock_count = 0
            
            for row in rows:
                value_idx = column_names.index('inventory_value') if 'inventory_value' in column_names else -1
                status_idx = column_names.index('stock_status') if 'stock_status' in column_names else -1
                
                if value_idx >= 0 and row[value_idx] is not None:
                    total_value += float(row[value_idx])
                
                if status_idx >= 0:
                    status = str(row[status_idx])
                    if 'éœ€è¦è¡¥è´§' in status:
                        low_stock_count += 1
                    elif 'åº“å­˜å……è¶³' in status:
                        high_stock_count += 1
                    else:
                        normal_stock_count += 1
            
            insight += f"â€¢ æ€»åº“å­˜ä»·å€¼ï¼šÂ¥{total_value:,.2f}\n"
            insight += f"â€¢ éœ€è¦è¡¥è´§ï¼š{low_stock_count}ç§äº§å“\n"
            insight += f"â€¢ åº“å­˜æ­£å¸¸ï¼š{normal_stock_count}ç§äº§å“\n"
            insight += f"â€¢ åº“å­˜å……è¶³ï¼š{high_stock_count}ç§äº§å“\n"
            
            return insight
            
        except Exception as e:
            return f"åº“å­˜æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _generate_trend_insight(self, rows: List[Tuple], column_names: List[str]) -> str:
        """ç”Ÿæˆè¶‹åŠ¿æ´å¯Ÿ"""
        try:
            insight = ""
            
            if len(rows) >= 2:
                amount_idx = column_names.index('monthly_sales_amount') if 'monthly_sales_amount' in column_names else -1
                if amount_idx >= 0:
                    current = float(rows[0][amount_idx]) if rows[0][amount_idx] is not None else 0
                    previous = float(rows[1][amount_idx]) if rows[1][amount_idx] is not None else 0
                    
                    if previous > 0:
                        growth = ((current - previous) / previous) * 100
                        insight += f"â€¢ ç¯æ¯”å¢é•¿ç‡ï¼š{growth:+.1f}%\n"
                    
                    insight += f"â€¢ å½“å‰æœˆé”€å”®é¢ï¼šÂ¥{current:,.2f}\n"
                    insight += f"â€¢ ä¸Šæœˆé”€å”®é¢ï¼šÂ¥{previous:,.2f}\n"
            
            return insight
            
        except Exception as e:
            return f"è¶‹åŠ¿æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _generate_general_insight(self, rows: List[Tuple], column_names: List[str], data_summary: str) -> str:
        """ç”Ÿæˆé€šç”¨æ´å¯Ÿ"""
        try:
            insight = f"â€¢ {data_summary}\n"
            
            # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
            if rows:
                insight += f"â€¢ æ•°æ®æ—¶é—´èŒƒå›´ï¼šæœ€æ–°è®°å½•åŒ…å«{len(rows)}æ¡æ•°æ®\n"
                
                # æŸ¥æ‰¾å¯èƒ½çš„æ•°å€¼åˆ—è¿›è¡Œç»Ÿè®¡
                numeric_cols = []
                for i, col in enumerate(column_names):
                    try:
                        values = [float(row[i]) for row in rows if row[i] is not None]
                        if values:
                            numeric_cols.append((col, sum(values)))
                    except:
                        continue
                
                if numeric_cols:
                    top_col = max(numeric_cols, key=lambda x: x[1])
                    insight += f"â€¢ ä¸»è¦æ•°å€¼å­—æ®µï¼š{top_col[0]} (æ€»è®¡{top_col[1]:,.2f})\n"
            
            return insight
            
        except Exception as e:
            return f"é€šç”¨æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def get_database_summary(self) -> str:
        """è·å–æ•°æ®åº“æ•´ä½“æ‘˜è¦"""
        try:
            summary = []
            summary.append(f"æ•°æ®åº“è¿æ¥ï¼š{PG_HOST}:{PG_PORT}/{PG_NAME}")
            summary.append(f"è¡¨æ•°é‡ï¼š{len(self.schema_analyzer.schema_info)}")
            
            # ç»Ÿè®¡æ¯ä¸ªè¡¨çš„æ•°æ®é‡
            for table_name in self.schema_analyzer.schema_info.keys():
                try:
                    cursor = self.conn.cursor()
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = cursor.fetchone()[0]
                    cursor.close()
                    summary.append(f"  {table_name}: {count} æ¡è®°å½•")
                except Exception:
                    summary.append(f"  {table_name}: æ— æ³•è·å–è®°å½•æ•°")
            
            return "\n".join(summary)
        except Exception as e:
            return f"æ•°æ®åº“æ‘˜è¦è·å–å¤±è´¥: {str(e)}"
    
    def close(self):
        self.conn.close()

    def build_sql_from_intent(self, intent: Dict) -> str:
        """æ ¹æ®æ„å›¾ç»“æ„è‡ªåŠ¨ç”ŸæˆSQLï¼Œåˆå¹¶æ‰€æœ‰SQLç”Ÿæˆé€»è¾‘"""
        query_type = intent.get("query_type", "ç»¼åˆæŸ¥è¯¢")
        filter_conditions = intent.get("filter_conditions", {})
        aggregation = intent.get("aggregation", {})
        order_by = aggregation.get("order_by", [])
        columns = intent.get("target_columns") or []
        # åªæŸ¥productè¡¨çš„ç®€å•æŸ¥è¯¢
        if query_type == "äº§å“åˆ†æ" and (not columns or set(columns) <= {"product_id","product_name","category","unit_price","cost_price","barcode"}):
            sql = """
            SELECT p.product_id, p.product_name, p.category, p.unit_price, p.cost_price, p.barcode
            FROM product p
            WHERE 1=1
            """
            if filter_conditions.get("category"):
                sql += f" AND p.category = '{filter_conditions['category']}'"
            if filter_conditions.get("product_name"):
                sql += f" AND p.product_name LIKE '%{filter_conditions['product_name']}%'"
            sql += " ORDER BY p.unit_price ASC LIMIT 20"
            return textwrap.dedent(sql)
        # å¤æ‚äº§å“åˆ†æï¼ˆå¸¦èšåˆï¼‰
        if query_type == "äº§å“åˆ†æ":
            sql = """
            SELECT 
                p.product_id,
                p.product_name,
                p.category,
                p.unit_price,
                p.cost_price,
                p.barcode,
                COALESCE(SUM(s.quantity), 0) as total_sales_quantity,
                COALESCE(SUM(s.total_amount), 0) as total_sales_amount,
                COALESCE(SUM(wi.quantity), 0) as total_warehouse_stock,
                COALESCE(SUM(si.stock_quantity), 0) as total_store_stock
            FROM product p
            LEFT JOIN sales s ON p.product_id = s.product_id
            LEFT JOIN warehouse_inventory wi ON p.product_id = wi.product_id
            LEFT JOIN store_inventory si ON p.product_id = si.product_id
            WHERE 1=1
            """
            if filter_conditions.get("category"):
                sql += f" AND p.category = '{filter_conditions['category']}'"
            if filter_conditions.get("product_name"):
                sql += f" AND p.product_name LIKE '%{filter_conditions['product_name']}%'"
            sql += " GROUP BY p.product_id, p.product_name, p.category, p.unit_price, p.cost_price, p.barcode"
            if "unit_price DESC" in order_by:
                sql += " ORDER BY p.unit_price DESC"
            elif "unit_price ASC" in order_by:
                sql += " ORDER BY p.unit_price ASC"
            elif "total_sales_quantity DESC" in order_by:
                sql += " ORDER BY total_sales_quantity DESC"
            elif "total_sales_amount DESC" in order_by:
                sql += " ORDER BY total_sales_amount DESC"
            else:
                sql += " ORDER BY p.product_name"
            sql += " LIMIT 20"
            return textwrap.dedent(sql)
        # å…¶å®ƒç±»å‹ï¼Œè°ƒç”¨åŸæœ‰ç”Ÿæˆæ–¹æ³•
        return self.generate_sql_from_intent(intent)

    def execute_query(self, sql: str) -> List[Tuple]:
        try:
            print(f"ğŸš€ æ‰§è¡ŒSQLæŸ¥è¯¢: {repr(sql)}")
            print(f"SQLç±»å‹: {type(sql)}")
            assert self.conn and self.conn.closed == 0, "æ•°æ®åº“è¿æ¥å·²å…³é—­"
            cursor = self.conn.cursor()
            cursor.execute(sql)
            rows = cursor.fetchall()
            cursor.close()
            print(f"âœ… æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(rows)} è¡Œæ•°æ®")
            return rows
        except Exception as e:
            print(f"âŒ SQLæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            with open("sql_error.log", "a", encoding="utf-8") as f:
                f.write(f"SQLæ‰§è¡Œå¤±è´¥: {repr(sql)}\\né”™è¯¯: {e}\\n")
            return []

    def query(self, question: str, context: str = "") -> str:
        """ä¸»å…¥å£ï¼šæ¥æ”¶ç”¨æˆ·é—®é¢˜ï¼Œè¿”å›ç›´è§‚åŒ–ä¸šåŠ¡æ•°æ®å’Œåˆ†æï¼Œä¸æ˜¾ç¤ºSQL"""
        # 1. æ„å›¾è¯†åˆ«
        intent = self.analyze_query_intent(question)
        print(f"âœ… æŸ¥è¯¢æ„å›¾åˆ†æå®Œæˆ: {intent}")

        # 2. SQLç”Ÿæˆ
        sql = self.build_sql_from_intent(intent)
        print(f"ğŸ”§ åŸºäºæ„å›¾ç”ŸæˆSQL: {intent}")  # ä»…æ—¥å¿—ï¼Œä¸è¾“å‡ºSQLå†…å®¹

        # 3. SQLæ‰§è¡Œ
        rows, column_names = self.execute_query_with_columns(sql)
        if rows:
            # 4. ä¸šåŠ¡æ´å¯Ÿä¸ç›´è§‚åŒ–è¾“å‡º
            answer = self._format_query_result(rows, column_names)
            insight = self._generate_comprehensive_insight(question, rows, column_names, sql)
            return f"{answer}\n{insight}"
        else:
            # 5. æ•°æ®ä¸ºç©ºæ—¶æ‰è€ƒè™‘çŸ¥è¯†åº“/LLMè¡¥å……
            kb_result = self.query_knowledge_base(question)
            return f"æœªæŸ¥è¯¢åˆ°ç›¸å…³æ•°æ®åº“æ•°æ®ã€‚\n{kb_result}"

    def _format_query_result(self, rows, column_names):
        """å°†æ•°æ®åº“æŸ¥è¯¢ç»“æœæ ¼å¼åŒ–ä¸ºç»“æ„åŒ–è¡¨æ ¼æˆ–ç›´è§‚æ–‡æœ¬"""
        if not rows or not column_names:
            return "æœªæŸ¥è¯¢åˆ°ç›¸å…³æ•°æ®ã€‚"
        # ç®€å•è¡¨æ ¼è¾“å‡º
        from tabulate import tabulate
        table = tabulate(rows, headers=column_names, tablefmt="fancy_grid", floatfmt=".2f")
        return f"\nğŸ“Š æŸ¥è¯¢ç»“æœ\n{table}\n"

class InMemoryKnowledgeBase:
    def __init__(self):
        self.documents: List[Document] = []
        self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        self.vectorstore = None
        self.db_agent = None  # æ·»åŠ æ•°æ®åº“Agentå¼•ç”¨

    def set_db_agent(self, db_agent):
        """è®¾ç½®æ•°æ®åº“Agentå¼•ç”¨"""
        self.db_agent = db_agent

    def load_from_postgres(self):
        """åŠ¨æ€åŠ è½½PostgreSQLæ•°æ®åˆ°çŸ¥è¯†åº“"""
        try:
            conn = psycopg2.connect(
                host=PG_HOST, port=PG_PORT, database=PG_NAME, user=PG_USER, password=PG_PASSWORD
            )
            schema_analyzer = DatabaseSchemaAnalyzer(conn)
            
            # ä¸ºæ¯ä¸ªè¡¨ç”ŸæˆçŸ¥è¯†ç‰‡æ®µ
            for table_name, columns in schema_analyzer.schema_info.items():
                try:
                    # è·å–è¡¨çš„å‰100è¡Œæ•°æ®ä½œä¸ºç¤ºä¾‹ï¼ˆå¢åŠ æ•°æ®é‡ï¼‰
                    cursor = conn.cursor()
                    cursor.execute(f"SELECT * FROM {table_name} LIMIT 100")
                    rows = cursor.fetchall()
                    cursor.close()
                    
                    if rows:
                        # ç”Ÿæˆè¡¨ç»“æ„æè¿°
                        col_names = [col['name'] for col in columns]
                        table_desc = f"è¡¨ {table_name} åŒ…å«å­—æ®µï¼š{', '.join(col_names)}"
                        self.documents.append(Document(
                            page_content=table_desc,
                            metadata={"type": "table_schema", "table": table_name}
                        ))
                        
                        # ç”Ÿæˆæ•°æ®ç¤ºä¾‹ï¼ˆå¢åŠ æ›´å¤šè¡Œï¼‰
                        for i, row in enumerate(rows[:10]):  # å¢åŠ åˆ°10è¡Œ
                            data_desc = f"{table_name}è¡¨æ•°æ®ç¤ºä¾‹{i+1}ï¼š{dict(zip(col_names, row))}"
                            self.documents.append(Document(
                                page_content=data_desc,
                                metadata={"type": "table_data", "table": table_name, "row": i+1}
                            ))
                        
                        # ç”Ÿæˆè¡¨ç»Ÿè®¡ä¿¡æ¯
                        try:
                            cursor = conn.cursor()
                            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                            total_count = cursor.fetchone()[0]
                            cursor.close()
                            
                            # ä¸ºæ•°å€¼åˆ—ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
                            numeric_cols = [col['name'] for col in columns if 'int' in col['type'] or 'decimal' in col['type'] or 'float' in col['type']]
                            if numeric_cols:
                                for col in numeric_cols[:3]:  # é™åˆ¶ç»Ÿè®¡åˆ—æ•°
                                    try:
                                        cursor = conn.cursor()
                                        cursor.execute(f"SELECT AVG({col}), MIN({col}), MAX({col}) FROM {table_name} WHERE {col} IS NOT NULL")
                                        stats = cursor.fetchone()
                                        cursor.close()
                                        if stats and stats[0] is not None:
                                            stats_desc = f"{table_name}è¡¨{col}å­—æ®µç»Ÿè®¡ï¼šå¹³å‡{stats[0]:.2f}, æœ€å°{stats[1]}, æœ€å¤§{stats[2]}, æ€»è®°å½•{total_count}"
                                            self.documents.append(Document(
                                                page_content=stats_desc,
                                                metadata={"type": "table_stats", "table": table_name, "column": col}
                                            ))
                                    except Exception:
                                        continue
                        except Exception:
                            pass
                
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†è¡¨ {table_name} æ—¶å‡ºé”™: {e}")
                    continue
            
            conn.close()
            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.documents)} ä¸ªæ•°æ®åº“çŸ¥è¯†ç‰‡æ®µ")
        except Exception as e:
            print(f"âŒ æ•°æ®åº“çŸ¥è¯†åŠ è½½å¤±è´¥: {e}")

    def get_realtime_data_context(self, question: str) -> str:
        """è·å–å®æ—¶æ•°æ®åº“æ•°æ®ä¸Šä¸‹æ–‡"""
        if not self.db_agent:
            return ""
        
        try:
            # ä½¿ç”¨æ•°æ®åº“Agentè¿›è¡Œå®æ—¶æŸ¥è¯¢
            db_result = self.db_agent.query(question)
            if db_result and "æœªæ‰¾åˆ°ç›¸å…³æ•°æ®" not in db_result:
                return f"å®æ—¶æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼š\n{db_result}"
        except Exception as e:
            print(f"âš ï¸ å®æ—¶æ•°æ®æŸ¥è¯¢å¤±è´¥: {e}")
        
        return ""

    def query_with_database_context(self, question: str) -> str:
        """ç»“åˆæ•°æ®åº“ä¸Šä¸‹æ–‡çš„çŸ¥è¯†åº“æŸ¥è¯¢"""
        try:
            # 1. è·å–çŸ¥è¯†åº“æ£€ç´¢ç»“æœ
            if not self.vectorstore:
                return "çŸ¥è¯†åº“æœªåˆå§‹åŒ–"
            
            docs = self.vectorstore.similarity_search(question, k=5)
            knowledge_context = self._format_knowledge_context(docs)
            
            # 2. è·å–å®æ—¶æ•°æ®åº“ä¸Šä¸‹æ–‡
            realtime_context = self.get_realtime_data_context(question)
            
            # 3. ç»“åˆåˆ†æ
            if realtime_context:
                combined_context = f"{knowledge_context}\n\n{realtime_context}"
            else:
                combined_context = knowledge_context
            
            return combined_context
            
        except Exception as e:
            return f"çŸ¥è¯†åº“æŸ¥è¯¢å¤±è´¥: {str(e)}"

    def _format_knowledge_context(self, docs: List[Document]) -> str:
        """æ ¼å¼åŒ–çŸ¥è¯†åº“ä¸Šä¸‹æ–‡"""
        if not docs:
            return ""
        
        formatted_contexts = []
        for i, doc in enumerate(docs[:3]):
            content = doc.page_content.strip()
            # æ¸…ç†å’Œæ ¼å¼åŒ–æ–‡æœ¬
            content = re.sub(r'\n+', ' ', content)
            content = re.sub(r'\s+', ' ', content)
            content = content[:400] + "..." if len(content) > 400 else content
            
            # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
            metadata_info = ""
            if doc.metadata.get("type") == "table_schema":
                metadata_info = f" [è¡¨ç»“æ„]"
            elif doc.metadata.get("type") == "table_data":
                metadata_info = f" [æ•°æ®ç¤ºä¾‹]"
            elif doc.metadata.get("type") == "table_stats":
                metadata_info = f" [ç»Ÿè®¡ä¿¡æ¯]"
            elif doc.metadata.get("type") == "pdf":
                metadata_info = f" [PDF: {doc.metadata.get('source', 'unknown')}]"
            
            formatted_contexts.append(f"çŸ¥è¯†ç‰‡æ®µ{i+1}{metadata_info}: {content}")
        
        return "\n".join(formatted_contexts)

    def load_from_pdfs(self, pdf_dir=PDF_DIR):
        if not os.path.exists(pdf_dir):
            print(f"âš ï¸ PDFç›®å½•ä¸å­˜åœ¨: {pdf_dir}")
            return
        for fname in os.listdir(pdf_dir):
            if not fname.lower().endswith('.pdf'):
                continue
            path = os.path.join(pdf_dir, fname)
            try:
                doc = fitz.open(path)
                for page_num in range(len(doc)):
                    text = doc.load_page(page_num).get_text()
                    if text.strip():
                        self.documents.append(Document(
                            page_content=text,
                            metadata={"type": "pdf", "source": fname, "page": page_num + 1}
                        ))
                doc.close()
            except Exception as e:
                print(f"âŒ è§£æPDFå¤±è´¥: {fname} {e}")

    def build_vectorstore(self):
        if not self.documents:
            raise RuntimeError("æ²¡æœ‰çŸ¥è¯†ç‰‡æ®µå¯ç”¨äºå‘é‡åŒ–")
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = splitter.split_documents(self.documents)
        self.vectorstore = FAISS.from_documents(docs, self.embeddings)

    def cleanup(self):
        self.documents.clear()
        self.vectorstore = None
class PDFMultiAgent:
    """PDF Agentï¼Œæ”¯æŒå¤šæ–‡æ¡£æ£€ç´¢"""
    def __init__(self, kb: InMemoryKnowledgeBase):
        self.kb = kb
    def query(self, question: str) -> str:
        docs = [d for d in self.kb.documents if d.metadata.get("type") == "pdf"]
        if not docs:
            return "æ— PDFçŸ¥è¯†åº“"
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        pdf_chunks = splitter.split_documents(docs)
        embeddings = self.kb.embeddings
        vectorstore = FAISS.from_documents(pdf_chunks, embeddings)
        results = vectorstore.similarity_search(question, k=3)
        if results:
            return "\n\n".join([r.page_content[:200] for r in results])
        return "æœªæ‰¾åˆ°ç›¸å…³PDFå†…å®¹"
class MemoryAgent:
    """è®°å¿†Agent - è´Ÿè´£ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå¯¹è¯å†å²ç®¡ç†"""
    def __init__(self, max_memory_size=10):
        self.conversation_history = deque(maxlen=max_memory_size)
        self.context_summary = ""
        self.llm = ChatOpenAI(
            model_name=os.getenv("MODEL_NAME", "gpt-4.1"),
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=os.getenv("OPENAI_API_URL"),
            temperature=0.3
        )
    def add_interaction(self, question: str, answer: str, context: str = ""):
        """æ·»åŠ å¯¹è¯äº¤äº’åˆ°è®°å¿†"""
        interaction = {
            "question": question,
            "answer": answer,
            "context": context,
            "timestamp": len(self.conversation_history) + 1
        }
        self.conversation_history.append(interaction)
        self._update_context_summary()
    def _update_context_summary(self):
        """æ›´æ–°ä¸Šä¸‹æ–‡æ‘˜è¦"""
        if not self.conversation_history:
            self.context_summary = ""
            return
        
        try:
            recent_interactions = list(self.conversation_history)[-3:]  # æœ€è¿‘3æ¬¡äº¤äº’
            summary_prompt = PromptTemplate.from_template("""
åŸºäºä»¥ä¸‹æœ€è¿‘çš„å¯¹è¯å†å²ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œç”¨äºç†è§£ç”¨æˆ·çš„è¿ç»­é—®é¢˜ï¼š
å¯¹è¯å†å²ï¼š
{history}
è¯·ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œçªå‡ºå…³é”®ä¿¡æ¯å’Œç”¨æˆ·å…³æ³¨ç‚¹ï¼š
""")
            
            history_text = "\n".join([
                f"Q{i+1}: {interaction['question']}\nA{i+1}: {interaction['answer'][:100]}..."
                for i, interaction in enumerate(recent_interactions)
            ])
            
            response = self.llm.invoke(summary_prompt.format(history=history_text))
            self.context_summary = response.content.strip()
            
        except Exception as e:
            print(f"âš ï¸ ä¸Šä¸‹æ–‡æ‘˜è¦æ›´æ–°å¤±è´¥: {e}")
            self.context_summary = ""
    
    def get_context_for_query(self, current_question: str) -> str:
        """ä¸ºå½“å‰æŸ¥è¯¢è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        if not self.conversation_history:
            return ""
        # æ£€æŸ¥å½“å‰é—®é¢˜æ˜¯å¦æ¶‰åŠä¹‹å‰çš„ä¸Šä¸‹æ–‡
        context_keywords = ["ç»“åˆ", "åŸºäº", "æ ¹æ®", "ä¹‹å‰", "ä¸Šè¿°", "å‰é¢", "ç¬¬ä¸€ä¸ªé—®é¢˜"]
        has_context_reference = any(keyword in current_question for keyword in context_keywords)
        
        if has_context_reference and self.context_summary:
            return f"å¯¹è¯ä¸Šä¸‹æ–‡ï¼š{self.context_summary}\n"
        
        return ""
    def clear_memory(self):
        """æ¸…ç©ºè®°å¿†"""
        self.conversation_history.clear()
        self.context_summary = ""

class DrawingAgent:
    """ç»˜å›¾Agent - è´Ÿè´£ç”Ÿæˆå¹¶æ‰§è¡Œç»˜å›¾ä»£ç ï¼ˆèåˆæœ¬åœ°å’Œä¸»æµç¨‹ä¼˜ç‚¹ï¼Œæ”¯æŒä¸­è‹±æ–‡promptï¼‰"""
    def __init__(self):
        self.llm = ChatOpenAI(
            model_name=os.getenv("MODEL_NAME", "gpt-4.1"),
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=os.getenv("OPENAI_API_URL"),
            temperature=0.4
        )

    def _extract_code(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–Pythonä»£ç å—"""
        if '```python' in text:
            start = text.find('```python') + len('```python')
            end = text.find('```', start)
            return text[start:end].strip()
        elif '```' in text:
            start = text.find('```') + 3
            end = text.find('```', start)
            return text[start:end].strip()
        return text

    def draw(self, question: str, data_context: str = "") -> str:
        """æ ¹æ®é—®é¢˜å’Œæ•°æ®ä¸Šä¸‹æ–‡ç”Ÿæˆå¹¶æ‰§è¡Œç»˜å›¾ä»£ç ï¼Œä¼˜å…ˆç”¨æ•°æ®åº“æ•°æ®ï¼Œå¤±è´¥é™çº§ä¸ºç¤ºä¾‹æ•°æ®"""
        timestamp = int(time.time())
        plot_filename = f"plot_{timestamp}.png"
        # æ”¯æŒä¸­è‹±æ–‡prompt
        if data_context:
            plot_context = f"""
è¯·ä½¿ç”¨ä»¥ä¸‹JSONæ ¼å¼çš„æ•°æ®è¿›è¡Œç»˜å›¾ï¼Œä¸è¦è‡ªå·±ç¼–é€ æ•°æ®ï¼š
--- DATA START ---
{data_context}
--- DATA END ---
"""
        else:
            plot_context = ""
        plot_prompt_template = PromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªæ•°æ®å¯è§†åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œæä¾›çš„æ•°æ®ï¼Œç”Ÿæˆä¸€æ®µå®Œæ•´çš„Pythonä»£ç æ¥ç»˜åˆ¶å›¾è¡¨ã€‚

{plot_context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

ä»£ç è¦æ±‚ï¼š
1. ä½¿ç”¨matplotlib.pyplotåº“ï¼Œå¹¶å°†å…¶åˆ«åä¸ºpltã€‚
2. åœ¨è°ƒç”¨plt.show()ä¹‹å‰ï¼Œå¿…é¡»å°†å›¾è¡¨ä¿å­˜åˆ°åä¸º'{plot_filename}'çš„æ–‡ä»¶ä¸­ã€‚
3. æœ€åå¿…é¡»è°ƒç”¨plt.show()æ¥æ˜¾ç¤ºå›¾åƒã€‚
4. ä»£ç å¿…é¡»æ˜¯å®Œæ•´ä¸”å¯ä»¥ç›´æ¥è¿è¡Œçš„ã€‚
5. å›¾è¡¨çš„æ ‡ç­¾ã€æ ‡é¢˜è¯·ç”¨è‹±æ–‡ï¼Œé¿å…ä¹±ç ã€‚
6. å¦‚æœæä¾›äº†æ•°æ®ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨æä¾›çš„æ•°æ®åº“æ•°æ®ã€‚å¦‚æœæä¾›äº†SQLæŸ¥è¯¢è¯­å¥ï¼Œåœ¨æ•°æ®åº“ä¸­è¿›è¡ŒæŸ¥è¯¢æ ¹æ®æŸ¥è¯¢åˆ°çš„å…·ä½“æ•°æ®è¿›è¡Œç”»å›¾ã€‚
7. å¦‚æœæ²¡æœ‰æä¾›æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨åˆç†ã€ç®€æ´çš„ç¤ºä¾‹æ•°æ®ã€‚
7. ç»™å›¾è¡¨æ·»åŠ åˆé€‚çš„æ ‡é¢˜(Title)å’Œåæ ‡è½´æ ‡ç­¾(X/Y Label)ã€‚
8. åœ¨å›¾è¡¨åº•éƒ¨ä¸­å¿ƒä½ç½®æ·»åŠ æ³¨é‡Šï¼š'Note: Data is for reference only.'
9. åªè¿”å›Pythonä»£ç å—ï¼Œç”¨```python ... ```åŒ…å›´ï¼Œä¸è¦ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚
""")
        final_prompt = plot_prompt_template.format(question=question, plot_context=plot_context, plot_filename=plot_filename)
        attempt = 0
        max_attempts = 5
        conversation = [{"role": "system", "content": "You are a helpful AI assistant that generates Python code for plotting graphs using matplotlib."}]
        conversation.append({"role": "user", "content": final_prompt})
        while attempt < max_attempts:
            attempt += 1
            print(f"\n[ç»˜å›¾å°è¯• {attempt}/{max_attempts}] æ­£åœ¨å‘LLMè¯·æ±‚ç»˜å›¾ä»£ç ...")
            response = self.llm.invoke(conversation)
            ai_response = response.content.strip()
            code = self._extract_code(ai_response)
            if not code:
                print(f"âŒ ç»˜å›¾å¤±è´¥: LLMæœªè¿”å›æœ‰æ•ˆçš„ä»£ç ã€‚")
                conversation.append({"role": "assistant", "content": ai_response})
                conversation.append({"role": "user", "content": "ä½ æ²¡æœ‰è¿”å›ä»»ä½•ä»£ç ã€‚è¯·åªè¿”å›è¢«```pythonåŒ…å›´çš„ä»£ç å—ã€‚"})
                continue
            # æ¸…ç†ä»£ç ï¼Œç§»é™¤ä¸å¿…è¦çš„åç«¯è®¾ç½®å’Œå¤šä½™çš„plt.show/plt.savefig
            code = code.replace("matplotlib.use('Agg')", "")
            code = code.replace("plt.show()", "")
            code = re.sub(r"plt\\.savefig\\s*\\(['\"].*?['\"]\\)", "", code, flags=re.DOTALL)
            # å¼ºåˆ¶æ·»åŠ ä¿å­˜å’Œæ˜¾ç¤ºå‘½ä»¤
            code += f"\n\n# Adding save and show commands by the system #wh_add_draw\n"
            code += f"plt.savefig('{plot_filename}', dpi=300, bbox_inches='tight') #wh_add_draw\n"
            code += f"plt.show() #wh_add_draw\n"
            script_name = f"temp_plot_{timestamp}_{attempt}.py"
            with open(script_name, "w", encoding="utf-8") as f:
                f.write(code)
            try:
                # å…¼å®¹Windowsç¼–ç 
                result = subprocess.run(
                    [sys.executable, script_name],
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    timeout=30,
                    env={**os.environ, 'PYTHONIOENCODING': 'utf-8'}
                )
                if result.returncode == 0 and os.path.exists(plot_filename):
                    print(f"âœ… ç»˜å›¾æˆåŠŸ! å›¾åƒå·²ä¿å­˜åˆ°: {os.path.abspath(plot_filename)}")
                    os.remove(script_name)
                    return f"ç»˜å›¾æˆåŠŸï¼Œæ–‡ä»¶ä¿å­˜åœ¨: {os.path.abspath(plot_filename)}"
                else:
                    error_msg = f"ä»£ç æ‰§è¡Œå¤±è´¥æˆ–æœªç”Ÿæˆå›¾åƒæ–‡ä»¶ã€‚\nReturn Code: {result.returncode}\nStderr: {result.stderr}"
                    print(f"âŒ {error_msg}")
                    conversation.append({"role": "assistant", "content": ai_response})
                    feedback = f"ä½ ç”Ÿæˆçš„ä»£ç æ‰§è¡Œå¤±è´¥äº†ï¼Œé”™è¯¯ä¿¡æ¯æ˜¯: {error_msg}ã€‚è¯·ä¿®å¤å®ƒå¹¶é‡æ–°ç”Ÿæˆå®Œæ•´çš„ä»£ç ã€‚"
                    conversation.append({"role": "user", "content": feedback})
            except subprocess.TimeoutExpired:
                error_msg = "æ‰§è¡Œè¶…æ—¶: ç»˜å›¾ä»£ç è¿è¡Œæ—¶é—´è¿‡é•¿ã€‚"
                print(f"âŒ {error_msg}")
                conversation.append({"role": "assistant", "content": ai_response})
                conversation.append({"role": "user", "content": f"ä½ ç”Ÿæˆçš„ä»£ç æ‰§è¡Œè¶…æ—¶äº†ã€‚è¯·ä¼˜åŒ–ä»£ç ï¼Œä½¿å…¶èƒ½å¿«é€Ÿæ‰§è¡Œã€‚"})
            except Exception as e:
                error_msg = f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                print(f"âŒ {error_msg}")
                os.remove(script_name)
                return f"ç»˜å›¾æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {error_msg}"
            finally:
                if os.path.exists(script_name):
                    os.remove(script_name)
        return f"âš ï¸ ç»è¿‡ {max_attempts} æ¬¡å°è¯•ï¼Œä»ç„¶æ— æ³•æˆåŠŸç”Ÿæˆå›¾åƒã€‚"

class TopAgent:
    """TopAgent - ä½œä¸ºä¸­æ¢å¤§è„‘ï¼Œè´Ÿè´£ç†è§£ã€åˆ†æå’ŒAgentåè°ƒ"""
    def __init__(self, memory_agent: MemoryAgent, db_agent, pdf_agent, kb, drawing_agent):
        self.memory_agent = memory_agent
        self.db_agent = db_agent
        self.pdf_agent = pdf_agent
        self.kb = kb
        self.drawing_agent = drawing_agent
        self.llm = ChatOpenAI(
            model_name=os.getenv("MODEL_NAME", "gpt-4.1"),
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=os.getenv("OPENAI_API_URL"),
            temperature=0.3
        )
        # åˆå§‹åŒ–è¯­ä¹‰æ£€ç´¢ç»„ä»¶
        self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        self.candidate_examples = self._initialize_candidate_examples()
        self.candidate_vectors = None
        self._build_candidate_vectors()
    
    def _initialize_candidate_examples(self) -> List[Dict]:
        """åˆå§‹åŒ–å€™é€‰ç¤ºä¾‹åº“"""
        examples = [
            {
                "task": "åº“å­˜åˆ†æ",
                "examples": [
                    "åˆ†æåº“å­˜ç‰©å“çš„ABCåˆ†ç±»",
                    "è®¡ç®—åº“å­˜å‘¨è½¬ç‡",
                    "è¯†åˆ«æ»é”€å•†å“",
                    "åˆ†æåº“å­˜ç»“æ„",
                    "è¯„ä¼°åº“å­˜æˆæœ¬"
                ]
            },
            {
                "task": "ä»“å‚¨è§„åˆ’",
                "examples": [
                    "ä¼˜åŒ–å­˜å‚¨ç­–ç•¥",
                    "è®¾è®¡è´§æ¶å¸ƒå±€",
                    "è§„åˆ’ä»“å‚¨ç©ºé—´",
                    "ç¡®å®šå­˜å‚¨ä½ç½®",
                    "åˆ†æå­˜å‚¨æ•ˆç‡"
                ]
            },
            {
                "task": "è®¢å•ç®¡ç†",
                "examples": [
                    "åˆ†æè®¢å•è¶‹åŠ¿",
                    "å¤„ç†è®¢å•å¼‚å¸¸",
                    "ä¼˜åŒ–è®¢å•æµç¨‹",
                    "ç»Ÿè®¡è®¢å•æ•°æ®",
                    "é¢„æµ‹è®¢å•é‡"
                ]
            },
            {
                "task": "ä¾›åº”é“¾åˆ†æ",
                "examples": [
                    "åˆ†æä¾›åº”å•†ç»©æ•ˆ",
                    "è¯„ä¼°ä¾›åº”é“¾é£é™©",
                    "ä¼˜åŒ–é‡‡è´­ç­–ç•¥",
                    "ç›‘æ§ä¾›åº”é“¾çŠ¶æ€",
                    "åˆ†æç‰©æµæˆæœ¬"
                ]
            },
            {
                "task": "æ•°æ®æŸ¥è¯¢",
                "examples": [
                    "æŸ¥è¯¢å•†å“ä¿¡æ¯",
                    "ç»Ÿè®¡é”€å”®æ•°æ®",
                    "åˆ†æå®¢æˆ·è¡Œä¸º",
                    "æŸ¥çœ‹åº“å­˜çŠ¶æ€",
                    "å¯¼å‡ºæŠ¥è¡¨æ•°æ®"
                ]
            }
        ]
        return examples
    def _build_candidate_vectors(self):
        """ç¦»çº¿æ„å»ºå€™é€‰ç¤ºä¾‹çš„å‘é‡è¡¨å¾"""
        try:
            all_examples = []
            for task_group in self.candidate_examples:
                for example in task_group["examples"]:
                    all_examples.append({
                        "text": example,
                        "task": task_group["task"],
                        "full_text": f"{task_group['task']}: {example}"
                    })
            # æ‰¹é‡ç”Ÿæˆå‘é‡è¡¨å¾
            texts = [item["full_text"] for item in all_examples]
            vectors = self.embeddings.embed_documents(texts)
            # å­˜å‚¨å‘é‡å’Œå…ƒæ•°æ®
            self.candidate_vectors = []
            for i, (item, vector) in enumerate(zip(all_examples, vectors)):
                self.candidate_vectors.append({
                    "id": i,
                    "text": item["text"],
                    "task": item["task"],
                    "full_text": item["full_text"],
                    "vector": vector
                })
            
            print(f"âœ… æˆåŠŸæ„å»º {len(self.candidate_vectors)} ä¸ªå€™é€‰ç¤ºä¾‹çš„å‘é‡è¡¨å¾")
        except Exception as e:
            print(f"âŒ å€™é€‰ç¤ºä¾‹å‘é‡æ„å»ºå¤±è´¥: {e}")
            self.candidate_vectors = []
    
    def _calculate_semantic_similarity(self, query_vector, candidate_vector) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰"""
        try:
            query_np = np.array(query_vector)
            candidate_np = np.array(candidate_vector)
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(query_np, candidate_np)
            query_norm = np.linalg.norm(query_np)
            candidate_norm = np.linalg.norm(candidate_np)
            if query_norm == 0 or candidate_norm == 0:
                return 0.0
            
            similarity = dot_product / (query_norm * candidate_norm)
            return float(similarity)
            
        except Exception as e:
            print(f"âš ï¸ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    def _knn_semantic_search(self, query: str, k: int = 5) -> List[Dict]:
        """åŸºäºKNNçš„è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢"""
        if not self.candidate_vectors:
            return []
        try:
            # å®æ—¶è¡¨å¾ç”¨æˆ·è¾“å…¥
            query_vector = self.embeddings.embed_query(query)
            # è®¡ç®—ä¸æ‰€æœ‰å€™é€‰ç¤ºä¾‹çš„ç›¸ä¼¼åº¦
            similarities = []
            for candidate in self.candidate_vectors:
                similarity = self._calculate_semantic_similarity(query_vector, candidate["vector"])
                similarities.append({
                    "candidate": candidate,
                    "similarity": similarity
                })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰kä¸ª
            similarities.sort(key=lambda x: x["similarity"], reverse=True)
            top_k_results = similarities[:k]
            
            return top_k_results
        except Exception as e:
            print(f"âš ï¸ KNNè¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
            return []
    def _enhance_query_with_semantic_context(self, query: str) -> str:
        """åŸºäºè¯­ä¹‰æ£€ç´¢å¢å¼ºæŸ¥è¯¢ä¸Šä¸‹æ–‡"""
        semantic_results = self._knn_semantic_search(query, k=3)
        
        if not semantic_results:
            return query
        # æ„å»ºè¯­ä¹‰ä¸Šä¸‹æ–‡
        context_parts = []
        for i, result in enumerate(semantic_results):
            candidate = result["candidate"]
            similarity = result["similarity"]
            if similarity > 0.5:  # åªä½¿ç”¨ç›¸ä¼¼åº¦è¾ƒé«˜çš„ç»“æœ
                context_parts.append(f"ç›¸å…³ä»»åŠ¡{i+1}: {candidate['task']} - {candidate['text']} (ç›¸ä¼¼åº¦: {similarity:.2f})")
        
        if context_parts:
            semantic_context = "\n".join(context_parts)
            enhanced_query = f"ç”¨æˆ·é—®é¢˜: {query}\n\nè¯­ä¹‰ç›¸å…³ä»»åŠ¡:\n{semantic_context}"
            return enhanced_query
        
        return query
    
    def analyze_query_intent(self, question: str, context: str = "") -> Dict:
        """åˆ†ææŸ¥è¯¢æ„å›¾ï¼Œå†³å®šéœ€è¦å“ªäº›Agentå‚ä¸"""
        try:
            intent_prompt = PromptTemplate.from_template("""
åˆ†æç”¨æˆ·é—®é¢˜çš„æ„å›¾ï¼Œå†³å®šéœ€è¦å“ªäº›ä¸“ä¸šAgentæ¥å›ç­”ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{question}
å¯¹è¯ä¸Šä¸‹æ–‡ï¼š{context}

è¯·åˆ†æé—®é¢˜ç±»å‹ï¼Œå¹¶è¿”å›JSONæ ¼å¼çš„å†³ç­–ï¼š
{{
    "requires_database": true/false,  // æ˜¯å¦éœ€è¦æ•°æ®åº“æŸ¥è¯¢è·å–æ•°æ®
    "requires_pdf": true/false,       // æ˜¯å¦éœ€è¦PDFæ£€ç´¢
    "requires_knowledge_base": true/false,  // æ˜¯å¦éœ€è¦çŸ¥è¯†åº“æ£€ç´¢
    "requires_drawing": true/false, // æ˜¯å¦éœ€è¦è°ƒç”¨ç»˜å›¾Agent
    "primary_agent": "database/pdf/knowledge_base/drawing/multi",  // ä¸»è¦Agent
    "reasoning": "åˆ†æç†ç”±"
}}

- å¦‚æœé—®é¢˜æ˜¯å…³äº"ç”»å›¾"ã€"ç»˜åˆ¶å›¾è¡¨"ã€"å¯è§†åŒ–"ã€"å›¾è¡¨"ã€"æŸ±çŠ¶å›¾"ã€"æŠ˜çº¿å›¾"ã€"é¥¼å›¾"ç­‰ï¼Œè¯·è®¾ç½® "requires_drawing": true å¹¶ä¸” "primary_agent": "drawing"ã€‚
- å¦‚æœç»˜å›¾éœ€è¦æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æ•°æ®ï¼ˆå¦‚"ç”»å‡ºæ¯ä¸ªä»“åº“çš„åº“å­˜é‡"ï¼‰ï¼Œè¯·åŒæ—¶è®¾ç½® "requires_database": trueã€‚
- å…¶ä»–æƒ…å†µç…§å¸¸åˆ†æã€‚

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
""")
            
            response = self.llm.invoke(intent_prompt.format(
                question=question,
                context=context
            ))
            
            # è§£æJSONå“åº”
            intent_data = json.loads(response.content.strip())
            return intent_data
            
        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç”»å›¾å…³é”®è¯
            drawing_keywords = ["ç”»å›¾", "ç»˜åˆ¶", "å›¾è¡¨", "å¯è§†åŒ–", "æŸ±çŠ¶å›¾", "æŠ˜çº¿å›¾", "é¥¼å›¾", "plot", "draw", "chart"]
            if any(keyword in question for keyword in drawing_keywords):
                return {
                    "requires_database": True,
                    "requires_pdf": False,
                    "requires_knowledge_base": False,
                    "requires_drawing": True,
                    "primary_agent": "drawing",
                    "reasoning": "å…³é”®è¯è§¦å‘ç»˜å›¾æ¨¡å¼"
                }
            # é»˜è®¤è¿”å›å¤šAgentæ¨¡å¼
            return {
                "requires_database": True,
                "requires_pdf": True,
                "requires_knowledge_base": True,
                "requires_drawing": False,
                "primary_agent": "multi",
                "reasoning": "é»˜è®¤å¤šAgentæ¨¡å¼"
            }
    
    def coordinate_agents(self, question: str, context: str = "") -> Dict:
        """åè°ƒå„ä¸ªAgentï¼Œè·å–ç»¼åˆå›ç­”"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç”»å›¾éœ€æ±‚
        drawing_keywords = ["ç”»å›¾", "ç»˜åˆ¶", "å›¾è¡¨", "å¯è§†åŒ–", "æŸ±çŠ¶å›¾", "æŠ˜çº¿å›¾", "é¥¼å›¾", "plot", "draw", "chart"]
        if any(keyword in question for keyword in drawing_keywords):
            print("ğŸ¨ æ£€æµ‹åˆ°ç”»å›¾éœ€æ±‚ï¼Œå¯åŠ¨ç»˜å›¾æµç¨‹...")
            db_data_context = ""
            data_summary = ""
            
            # æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦æ•°æ®åº“æ•°æ®
            # 1. æ˜ç¡®åŒ…å«æ•°æ®åº“ç›¸å…³å…³é”®è¯
            db_related_keywords = ["ä»“åº“", "åº“å­˜", "é”€å”®", "äº§å“", "é—¨åº—", "è¡¥è´§", "warehouse", "inventory", "sales", "product", "store"]
            is_db_related = any(keyword in question for keyword in db_related_keywords)
            
            # 2. æ£€æŸ¥æ˜¯å¦æ˜¯é€šç”¨ç”»å›¾éœ€æ±‚ï¼ˆå¦‚å†å²ã€åœ°ç†ç­‰ï¼‰
            general_keywords = ["å†å²", "æœä»£", "å›½å®¶", "åœ°ç†", "äººå£", "ç»æµ", "å†å²", "dynasty", "country", "geography", "population", "economy"]
            is_general = any(keyword in question for keyword in general_keywords)
            
            if is_db_related and not is_general:
                # é™é»˜è·å–æ•°æ®åº“æ•°æ®ï¼Œä¸æ˜¾ç¤ºæŠ€æœ¯ç»†èŠ‚
                try:
                    sql = self.db_agent.generate_sql(question)
                    if sql:
                        plot_data = self.db_agent.get_data_for_plotting(sql)
                        if plot_data and len(plot_data) > 0:
                            db_data_context = json.dumps(plot_data, ensure_ascii=False, indent=2)
                            # ç”Ÿæˆæ•°æ®æ‘˜è¦
                            data_summary = self._generate_data_summary_for_plot(plot_data, question)
                            print(f"âœ… æˆåŠŸè·å–æ•°æ®åº“æ•°æ®ç”¨äºç»˜å›¾ï¼Œå…±{len(plot_data)}æ¡è®°å½•")
                        else:
                            print("ğŸ“Š æ•°æ®åº“æ— ç›¸å…³æ•°æ®ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®")
                    else:
                        print("ğŸ“Š æ— æ³•ç”Ÿæˆæ•°æ®åº“æŸ¥è¯¢ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®")
                except Exception as e:
                    print(f"ğŸ“Š æ•°æ®åº“æŸ¥è¯¢å¼‚å¸¸ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®: {str(e)}")
            else:
                print("ğŸ“Š æ£€æµ‹åˆ°é€šç”¨ç”»å›¾éœ€æ±‚ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®...")
            
            # è°ƒç”¨ç»˜å›¾Agent
            plot_result = self.drawing_agent.draw(question, db_data_context)
            
            # æ„å»ºç”¨æˆ·å‹å¥½çš„å›ç­”
            if "æˆåŠŸ" in plot_result:
                if data_summary:
                    user_answer = f"ğŸ¨ å·²æ ¹æ®æ•°æ®åº“ä¿¡æ¯ç”Ÿæˆå›¾è¡¨\n\n{data_summary}\n\n{plot_result}"
                else:
                    user_answer = f"ğŸ¨ å·²ç”Ÿæˆå›¾è¡¨\n\n{plot_result}"
            else:
                user_answer = f"âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {plot_result}"
            
            return {
                "answer": user_answer,
                "knowledge_context": "",
                "db_result": data_summary if data_summary else "ä½¿ç”¨ç¤ºä¾‹æ•°æ®",
                "pdf_result": "",
                "source_type": "drawing_agent",
                "confidence": 0.95,
                "agent_decision": {
                    "primary_agent": "drawing",
                    "reasoning": "ç”¨æˆ·è¾“å…¥åŒ…å«ç”»å›¾å…³é”®è¯ï¼Œç›´æ¥è§¦å‘ç»˜å›¾æ¨¡å¼",
                    "requires_database": is_db_related and not is_general,
                    "requires_pdf": False,
                    "requires_knowledge_base": False,
                    "requires_drawing": True
                },
                "semantic_results": [],
                "plot_path": plot_result if "æˆåŠŸ" in plot_result else None
            }
        
        # 1. è¯­ä¹‰æ£€ç´¢å¢å¼º
        enhanced_question = self._enhance_query_with_semantic_context(question)
        semantic_results = self._knn_semantic_search(question, k=3)
        
        # æ£€æŸ¥æœ€é«˜ç›¸å…³æ€§
        max_similarity = max([r['similarity'] for r in semantic_results], default=0)
        
        # 2. åˆ†ææŸ¥è¯¢æ„å›¾
        try:
            intent = self.analyze_query_intent(enhanced_question, context)
        except Exception as e:
            print(f"âš ï¸ æ„å›¾åˆ†æå¤±è´¥: {e}")
            intent = None
        
        # å¦‚æœæ„å›¾åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
        if not intent or not isinstance(intent, dict) or not intent.get('primary_agent'):
            intent = {
                "requires_database": True,
                "requires_pdf": True,
                "requires_knowledge_base": True,
                "requires_drawing": False,
                "primary_agent": "multi",
                "reasoning": "é»˜è®¤å¤šAgentåè°ƒæ¨¡å¼"
            }
        
        # 3. ä¼˜å…ˆæ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
        results = {}
        db_result = ""
        
        # æ•°æ®åº“æŸ¥è¯¢ï¼ˆä¼˜å…ˆæ‰§è¡Œï¼‰
        if intent.get("requires_database", True):
            try:
                print("ğŸ” ä¼˜å…ˆæ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢...")
                # ç›´æ¥æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
                db_result = self.db_agent.query(question, context)
                
                # æ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢æ˜¯å¦æˆåŠŸè¿”å›å…·ä½“æ•°æ®
                if db_result and "æœªæ‰¾åˆ°ç›¸å…³æ•°æ®" not in db_result and "æ— æ³•ç†è§£æŸ¥è¯¢éœ€æ±‚" not in db_result:
                    results["db_result"] = db_result
                    print("âœ… æ•°æ®åº“æŸ¥è¯¢æˆåŠŸï¼Œè¿”å›å…·ä½“æ•°æ®")
                else:
                    print("âš ï¸ æ•°æ®åº“æŸ¥è¯¢æœªè¿”å›å…·ä½“æ•°æ®")
                    results["db_result"] = "æ•°æ®åº“æŸ¥è¯¢æœªè¿”å›å…·ä½“æ•°æ®"
                    
                # è·å–æ•°æ®åº“æ‘˜è¦ä¿¡æ¯
                try:
                    db_summary = self.db_agent.get_database_summary()
                    results["db_summary"] = db_summary
                except Exception:
                    pass
                    
            except Exception as e:
                print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
                results["db_result"] = f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}"
        
        # 4. å¦‚æœæ•°æ®åº“æŸ¥è¯¢æˆåŠŸè¿”å›å…·ä½“æ•°æ®ï¼Œç›´æ¥åŸºäºæ•°æ®ç”Ÿæˆå›ç­”
        if results.get("db_result") and "æœªæ‰¾åˆ°ç›¸å…³æ•°æ®" not in results["db_result"] and "æ— æ³•ç†è§£æŸ¥è¯¢éœ€æ±‚" not in results["db_result"]:
            print("ğŸ¯ åŸºäºæ•°æ®åº“å…·ä½“æ•°æ®ç”Ÿæˆå›ç­”...")
            
            # çŸ¥è¯†åº“æŸ¥è¯¢ï¼ˆä½œä¸ºè¡¥å……ï¼‰
            if intent.get("requires_knowledge_base", True):
                try:
                    if hasattr(self.kb, 'query_with_database_context'):
                        results["knowledge_context"] = self.kb.query_with_database_context(question)
                    else:
                        docs = self.kb.vectorstore.similarity_search(question, k=3)
                        results["knowledge_context"] = self._format_knowledge_context(docs)
                except Exception as e:
                    results["knowledge_context"] = f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}"
            
            # PDFæŸ¥è¯¢ï¼ˆä½œä¸ºè¡¥å……ï¼‰
            if intent.get("requires_pdf", True):
                try:
                    results["pdf_result"] = self.pdf_agent.query(question)
                except Exception as e:
                    results["pdf_result"] = f"PDFæ£€ç´¢å¤±è´¥: {e}"
            
            # åŸºäºæ•°æ®åº“å…·ä½“æ•°æ®ç”Ÿæˆæ™ºèƒ½å›ç­”
            final_answer = self._generate_data_driven_answer(question, results, intent, semantic_results)
            
            return {
                "answer": final_answer,
                "knowledge_context": results.get("knowledge_context", ""),
                "db_result": results.get("db_result", ""),
                "pdf_result": results.get("pdf_result", ""),
                "db_summary": results.get("db_summary", ""),
                "source_type": "database_driven",
                "confidence": min(0.95, 0.8 + max_similarity * 0.15),
                "agent_decision": intent,
                "semantic_results": semantic_results
            }
        
        # 5. å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿå¤šAgentæ¨¡å¼
        print("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿå¤šAgentåè°ƒæ¨¡å¼...")
        
        # çŸ¥è¯†åº“æŸ¥è¯¢
        if intent.get("requires_knowledge_base", True):
            try:
                if hasattr(self.kb, 'query_with_database_context'):
                    results["knowledge_context"] = self.kb.query_with_database_context(question)
                else:
                    docs = self.kb.vectorstore.similarity_search(question, k=5)
                    results["knowledge_context"] = self._format_knowledge_context(docs)
            except Exception as e:
                results["knowledge_context"] = f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}"
        
        # PDFæŸ¥è¯¢
        if intent.get("requires_pdf", True):
            try:
                results["pdf_result"] = self.pdf_agent.query(question)
            except Exception as e:
                results["pdf_result"] = f"PDFæ£€ç´¢å¤±è´¥: {e}"
        
        # æ™ºèƒ½ç»“æœæ•´åˆ
        final_answer = self._generate_intelligent_answer(question, results, intent, semantic_results)
        
        return {
            "answer": final_answer,
            "knowledge_context": results.get("knowledge_context", ""),
            "db_result": results.get("db_result", ""),
            "pdf_result": results.get("pdf_result", ""),
            "db_summary": results.get("db_summary", ""),
            "source_type": "top_agent_coordinated",
            "confidence": min(0.9, 0.7 + max_similarity * 0.2),
            "agent_decision": intent,
            "semantic_results": semantic_results
        }
    
    def _generate_data_driven_answer(self, question: str, results: Dict, intent: Dict, semantic_results: List) -> str:
        """åŸºäºæ•°æ®åº“å…·ä½“æ•°æ®ç”Ÿæˆå›ç­”"""
        try:
            # æ„å»ºæ•°æ®é©±åŠ¨çš„å›ç­”
            data_prompt = PromptTemplate.from_template("""
ä½œä¸ºæ™ºèƒ½ä»“å‚¨ç³»ç»Ÿçš„æ•°æ®åˆ†æä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹æ•°æ®åº“å…·ä½“æ•°æ®ï¼Œä¸ºç”¨æˆ·é—®é¢˜æä¾›ç›´æ¥ã€å‡†ç¡®ã€æ•°æ®é©±åŠ¨çš„å›ç­”ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{question}

ã€æ•°æ®åº“å…·ä½“æ•°æ®ã€‘
{db_result}

ã€çŸ¥è¯†åº“è¡¥å……ä¿¡æ¯ã€‘
{knowledge_context}

ã€PDFè¡¥å……ä¿¡æ¯ã€‘
{pdf_result}

è¯·æä¾›ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼ŒåŸºäºæ•°æ®åº“å…·ä½“æ•°æ®
2. æ•°æ®åˆ†æå’Œä¸šåŠ¡æ´å¯Ÿ
3. å…·ä½“çš„æ•°å€¼å’Œç»Ÿè®¡ä¿¡æ¯
4. åŸºäºæ•°æ®çš„å»ºè®®

è¦æ±‚ï¼š
- å›ç­”è¦åŸºäºæ•°æ®åº“çš„å…·ä½“æ•°æ®ï¼Œä¸è¦ç»™å‡ºSQLå»ºè®®
- çªå‡ºå…³é”®æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
- æä¾›æ•°æ®é©±åŠ¨çš„ä¸šåŠ¡æ´å¯Ÿ
- å›ç­”è¦ç®€æ´ã€ä¸“ä¸šã€å‡†ç¡®

åŸºäºæ•°æ®çš„å›ç­”ï¼š
""")
            
            response = self.llm.invoke(data_prompt.format(
                question=question,
                db_result=results.get("db_result", "æ— æ•°æ®åº“æ•°æ®"),
                knowledge_context=results.get("knowledge_context", "æ— çŸ¥è¯†åº“ä¿¡æ¯"),
                pdf_result=results.get("pdf_result", "æ— PDFä¿¡æ¯")
            ))
            
            return response.content.strip()
            
        except Exception as e:
            return f"æ•°æ®é©±åŠ¨å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _format_knowledge_context(self, docs: List[Document]) -> str:
        """æ ¼å¼åŒ–çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼Œè§£å†³å¤šè¡Œéš”æ–­é—®é¢˜"""
        if not docs:
            return ""
        
        formatted_contexts = []
        for i, doc in enumerate(docs[:3]):  # åªå–å‰3ä¸ªæœ€ç›¸å…³çš„
            content = doc.page_content.strip()
            # æ¸…ç†å’Œæ ¼å¼åŒ–æ–‡æœ¬
            content = re.sub(r'\n+', ' ', content)  # å°†å¤šä¸ªæ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
            content = re.sub(r'\s+', ' ', content)  # å°†å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
            content = content[:300] + "..." if len(content) > 300 else content
            formatted_contexts.append(f"çŸ¥è¯†ç‰‡æ®µ{i+1}: {content}")
        
        return "\n".join(formatted_contexts)
    
    def _generate_data_summary_for_plot(self, plot_data: List[Dict], question: str) -> str:
        """ä¸ºç»˜å›¾ç”Ÿæˆæ•°æ®æ‘˜è¦"""
        try:
            if not plot_data:
                return ""
            
            # åˆ†ææ•°æ®ç»“æ„
            sample_record = plot_data[0]
            columns = list(sample_record.keys())
            
            # ç”Ÿæˆæ‘˜è¦
            summary_parts = []
            summary_parts.append(f"ğŸ“Š æ•°æ®æ¦‚è§ˆï¼šåŸºäº {len(plot_data)} æ¡è®°å½•")
            
            # è¯†åˆ«å…³é”®å­—æ®µ
            numeric_fields = []
            categorical_fields = []
            
            for field in columns:
                if field in ['quantity', 'total_amount', 'unit_price', 'cost_price', 'stock_quantity', 'safety_stock']:
                    numeric_fields.append(field)
                elif field in ['product_name', 'category', 'warehouse_name', 'store_name']:
                    categorical_fields.append(field)
            
            # æ·»åŠ æ•°å€¼å­—æ®µç»Ÿè®¡
            if numeric_fields:
                for field in numeric_fields[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ•°å€¼å­—æ®µ
                    try:
                        values = [float(record[field]) for record in plot_data if record[field] is not None]
                        if values:
                            total = sum(values)
                            avg = total / len(values)
                            summary_parts.append(f"â€¢ {field}: æ€»è®¡ {total:,.2f}, å¹³å‡ {avg:.2f}")
                    except:
                        continue
            
            # æ·»åŠ åˆ†ç±»å­—æ®µç»Ÿè®¡
            if categorical_fields:
                for field in categorical_fields[:2]:  # æœ€å¤šæ˜¾ç¤º2ä¸ªåˆ†ç±»å­—æ®µ
                    try:
                        unique_values = set(record[field] for record in plot_data if record[field] is not None)
                        if unique_values:
                            summary_parts.append(f"â€¢ {field}: {len(unique_values)} ä¸ªä¸åŒå€¼")
                    except:
                        continue
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            return f"æ•°æ®æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"

    def _generate_intelligent_answer(self, question: str, results: Dict, intent: Dict, semantic_results: List) -> str:
        """æ™ºèƒ½ç”Ÿæˆç»¼åˆå›ç­”"""
        try:
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts = []
            
            # æ·»åŠ è¯­ä¹‰ç›¸å…³ä»»åŠ¡ä¿¡æ¯
            if semantic_results:
                relevant_tasks = []
                for result in semantic_results[:2]:  # å–å‰2ä¸ªæœ€ç›¸å…³çš„
                    if result['similarity'] > 0.4:
                        candidate = result['candidate']
                        relevant_tasks.append(f"{candidate['task']}: {candidate['text']}")
                
                if relevant_tasks:
                    context_parts.append(f"ç›¸å…³ä»»åŠ¡: {'; '.join(relevant_tasks)}")
            
            # æ·»åŠ æ•°æ®åº“æ‘˜è¦
            if results.get("db_summary"):
                context_parts.append(f"æ•°æ®åº“çŠ¶æ€: {results['db_summary']}")
            
            # æ„å»ºç»¼åˆæç¤º
            synthesis_prompt = PromptTemplate.from_template("""
ä½œä¸ºæ™ºèƒ½ä»“å‚¨ç³»ç»Ÿçš„ä¸­æ¢å¤§è„‘ï¼Œè¯·åŸºäºä»¥ä¸‹å¤šæºä¿¡æ¯ç”Ÿæˆä¸“ä¸šã€ç»“æ„åŒ–çš„ç»¼åˆå›ç­”ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{question}
Agentå†³ç­–ï¼š{intent_reasoning}
ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context_info}

ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘
{knowledge_context}

ã€æ•°æ®åº“åˆ†æã€‘
{db_result}

ã€PDFæ£€ç´¢ç»“æœã€‘
{pdf_result}

è¯·æä¾›ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
2. åŸºäºå¤šæºä¿¡æ¯çš„ç»¼åˆåˆ†æ
3. æ•°æ®é©±åŠ¨çš„ä¸šåŠ¡æ´å¯Ÿ
4. å…·ä½“çš„å»ºè®®å’Œä¼˜åŒ–æ–¹å‘
5. å¦‚æœæœ‰ä¸Šä¸‹æ–‡å…³è”ï¼Œè¯·ä½“ç°è¿ç»­æ€§

è¦æ±‚ï¼š
- å›ç­”è¦ç®€æ´ã€ä¸“ä¸šã€ç»“æ„åŒ–
- å……åˆ†åˆ©ç”¨æ•°æ®åº“çš„å…·ä½“æ•°æ®
- ç»“åˆçŸ¥è¯†åº“çš„ç†è®ºæŒ‡å¯¼
- ä½“ç°æ™ºèƒ½åˆ†æèƒ½åŠ›

ç»¼åˆå›ç­”ï¼š
""")
            
            context_info = "\n".join(context_parts) if context_parts else "æ— ç‰¹æ®Šä¸Šä¸‹æ–‡"
            
            response = self.llm.invoke(synthesis_prompt.format(
                question=question,
                intent_reasoning=intent.get("reasoning", ""),
                context_info=context_info,
                knowledge_context=results.get("knowledge_context", "æ— ç›¸å…³ä¿¡æ¯"),
                db_result=results.get("db_result", "æ— æ•°æ®åº“ç»“æœ"),
                pdf_result=results.get("pdf_result", "æ— PDFç»“æœ")
            ))
            
            return response.content.strip()
            
        except Exception as e:
            return f"æ™ºèƒ½å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}"

class AgenticRAGSystem:
    def __init__(self):
        # 1. åˆå§‹åŒ–çŸ¥è¯†åº“
        self.kb = InMemoryKnowledgeBase()
        
        # 2. åˆå§‹åŒ–æ•°æ®åº“Agent
        self.db_agent = UniversalDatabaseAgent()
        
        # 3. è®¾ç½®çŸ¥è¯†åº“çš„æ•°æ®åº“Agentå¼•ç”¨
        self.kb.set_db_agent(self.db_agent)
        
        # 4. åŠ è½½æ•°æ®åˆ°çŸ¥è¯†åº“
        self.kb.load_from_postgres()
        self.kb.load_from_pdfs()
        self.kb.build_vectorstore()
        
        # 5. åˆå§‹åŒ–å…¶ä»–Agent
        self.pdf_agent = PDFMultiAgent(self.kb)
        self.memory_agent = MemoryAgent()
        self.drawing_agent = DrawingAgent()  # æ·»åŠ ç»˜å›¾Agent
        self.top_agent = TopAgent(self.memory_agent, self.db_agent, self.pdf_agent, self.kb, self.drawing_agent)
        
        # 6. åˆå§‹åŒ–LLM
        self.llm = ChatOpenAI(
            model_name=os.getenv("MODEL_NAME"),
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=os.getenv("OPENAI_API_URL"),
            temperature=0.3
        )
        
        print("âœ… æ™ºèƒ½å¤šAgent RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š æ•°æ®åº“è¿æ¥: {PG_HOST}:{PG_PORT}/{PG_NAME}")
        print(f"ğŸ“š çŸ¥è¯†åº“æ–‡æ¡£æ•°: {len(self.kb.documents)}")
        print(f"ğŸ§  è¯­ä¹‰æ£€ç´¢å€™é€‰æ•°: {len(self.top_agent.candidate_vectors) if self.top_agent.candidate_vectors else 0}")

    def process_query(self, query: str) -> Dict:
        # 1. è·å–ä¸Šä¸‹æ–‡
        context = self.memory_agent.get_context_for_query(query)
        
        # 2. TopAgentåè°ƒå„ä¸ªAgent
        result = self.top_agent.coordinate_agents(query, context)
        
        # 3. æ›´æ–°è®°å¿†
        self.memory_agent.add_interaction(query, result["answer"], context)
        
        return result

    def close(self):
        self.kb.cleanup()
        self.db_agent.close()
        self.memory_agent.clear_memory()
        print("ğŸ§¹ ç³»ç»Ÿèµ„æºå·²æ¸…ç†")

# FastAPIæ¥å£
app = FastAPI(title="æ™ºèƒ½å¤šAgent RAG API")
class QueryRequest(BaseModel):
    question: str

@asynccontextmanager
async def lifespan(app: FastAPI):
    global rag
    rag = AgenticRAGSystem()
    yield
    rag.close()

app = FastAPI(lifespan=lifespan)

@app.post("/query")
def query_api(req: QueryRequest):
    try:
        result = rag.process_query(req.question)
        return result
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

def display_result(result: Dict):
    """æ ¼å¼åŒ–æ˜¾ç¤ºç»“æœï¼ˆä¸æ˜¾ç¤ºæ•°æ®åº“çŠ¶æ€ã€Agentå†³ç­–åˆ†æã€ä¿¡æ¯æ¥æºå’ŒSQLç›¸å…³å†…å®¹ï¼‰"""
    print("\n" + "="*50)
    print("ğŸ“ æ™ºèƒ½ç»“æ„åŒ–å›ç­”")
    print("="*50)
    print(result.get('answer', 'æ— å›ç­”'))
    
    # ç»˜å›¾ç»“æœç‰¹æ®Šå¤„ç†
    if result.get("source_type") == "drawing_agent":
        if result.get("plot_path"):
            print("âœ… å›¾åƒå·²å°è¯•åœ¨çª—å£ä¸­æ˜¾ç¤ºï¼Œå¹¶å·²ä¿å­˜åˆ°æœ¬åœ°ã€‚")
        # å¯¹äºç”»å›¾åŠŸèƒ½ï¼Œä¸æ˜¾ç¤ºå…¶ä»–æŠ€æœ¯ç»†èŠ‚
        return
    
    # ç½®ä¿¡åº¦å’Œç›¸å…³æ€§
    if 'confidence' in result:
        print(f"\nğŸ¯ ç½®ä¿¡åº¦: {result['confidence']:.1%}")
    if 'relevance_score' in result:
        print(f"ğŸ“Š çŸ¥è¯†åº“åŒ¹é…åº¦: {result['relevance_score']:.1%}")
    
    # è¯­ä¹‰æ£€ç´¢ç»“æœ
    if 'semantic_results' in result and result['semantic_results']:
        print("\nğŸ” è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢")
        print("-" * 20)
        for i, semantic_result in enumerate(result['semantic_results'][:3]):
            candidate = semantic_result['candidate']
            similarity = semantic_result['similarity']
            if similarity > 0.3:  # åªæ˜¾ç¤ºç›¸ä¼¼åº¦è¾ƒé«˜çš„ç»“æœ
                print(f"ç›¸å…³ä»»åŠ¡{i+1}: {candidate['task']} - {candidate['text']}")
                print(f"ç›¸ä¼¼åº¦: {similarity:.3f}")
    
    # è¯¦ç»†ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
    if 'knowledge_context' in result and result['knowledge_context']:
        print("\nğŸ§  çŸ¥è¯†åº“ç‰‡æ®µ:")
        print(result['knowledge_context'])
    if 'db_result' in result and result['db_result']:
        print("\nğŸ’¾ æ•°æ®åº“åˆ†æ:")
        print(result['db_result'])
    if 'pdf_result' in result and result['pdf_result']:
        print("\nğŸ“„ PDFæ£€ç´¢:")
        print(result['pdf_result'])

def main():
    print("ğŸš€ === æ™ºèƒ½å¤šAgent RAGä»“åº“ç®¡ç†ç³»ç»Ÿï¼ˆæ”¯æŒç”»å›¾åŠŸèƒ½ï¼‰ ===")
    print("ğŸ’¡ è¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢ï¼š")
    print("ğŸ¨ æ”¯æŒç”»å›¾åŠŸèƒ½ï¼šè¾“å…¥åŒ…å«'ç”»å›¾'ã€'ç»˜åˆ¶'ã€'å›¾è¡¨'ç­‰å…³é”®è¯çš„é—®é¢˜")
    print("ğŸ”š è¾“å…¥'é€€å‡º'ã€'quit'ã€'exit'æˆ–'q'ç»“æŸä¼šè¯")
    print("ğŸ§¹ è¾“å…¥'clear'æ¸…ç©ºå¯¹è¯è®°å¿†\n")
    
    rag = AgenticRAGSystem()
    try:
        while True:
            query = input("\nğŸ¤” è¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢> ").strip()
            if not query:
                continue
            if query.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ§¹ æ­£åœ¨æ¸…ç©ºå¯¹è¯è®°å¿†...")
                rag.memory_agent.clear_memory()
                print("âœ… å¯¹è¯è®°å¿†å·²æ¸…ç©º")
                break
            if query.lower() == 'clear':
                rag.memory_agent.clear_memory()
                print("ğŸ§¹ å¯¹è¯è®°å¿†å·²æ¸…ç©º")
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç”»å›¾éœ€æ±‚
            drawing_keywords = ["ç”»å›¾", "ç»˜åˆ¶", "å›¾è¡¨", "å¯è§†åŒ–", "æŸ±çŠ¶å›¾", "æŠ˜çº¿å›¾", "é¥¼å›¾", "plot", "draw", "chart"]
            is_drawing_request = any(keyword in query for keyword in drawing_keywords)
            
            if is_drawing_request:
                print("ğŸ¨ æ£€æµ‹åˆ°ç”»å›¾éœ€æ±‚ï¼Œå¯åŠ¨ç»˜å›¾æµç¨‹...")
                # ç”»å›¾åŠŸèƒ½é™é»˜å¤„ç†ï¼Œä¸æ˜¾ç¤ºæŠ€æœ¯ç»†èŠ‚
                import sys
                class DummyFile:
                    def write(self, x): 
                        # åªæ˜¾ç¤ºç”»å›¾ç›¸å…³çš„è¾“å‡º
                        if any(keyword in x for keyword in ["ğŸ¨", "ğŸ“Š", "âœ…", "âŒ", "ç»˜å›¾", "ç”»å›¾", "å›¾åƒ", "å›¾è¡¨"]):
                            sys.__stdout__.write(x)
                old_stdout = sys.stdout
                sys.stdout = DummyFile()
                result = rag.process_query(query)
                sys.stdout = old_stdout
            else:
                # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºSQLç›¸å…³æ—¥å¿—
                import os
                if os.getenv('RAG_DEBUG', '0') == '1':
                    result = rag.process_query(query)
                else:
                    # ä¸´æ—¶å±è”½SQLç›¸å…³print
                    import sys
                    class DummyFile:
                        def write(self, x): pass
                    old_stdout = sys.stdout
                    sys.stdout = DummyFile()
                    result = rag.process_query(query)
                    sys.stdout = old_stdout
            
            display_result(result)
    finally:
        rag.close()
        print("\nğŸ‘‹ ç³»ç»Ÿå·²å…³é—­")

# Flaské›†æˆæ”¯æŒ
def create_rag_app():
    """åˆ›å»ºFlaské›†æˆçš„RAGåº”ç”¨å®ä¾‹"""
    return AgenticRAGSystem()

# ç”»å›¾åŠŸèƒ½æ”¯æŒ
def is_drawing_request(question: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯ç”»å›¾è¯·æ±‚"""
    drawing_keywords = ["ç”»å›¾", "ç»˜åˆ¶", "å›¾è¡¨", "å¯è§†åŒ–", "æŸ±çŠ¶å›¾", "æŠ˜çº¿å›¾", "é¥¼å›¾", "plot", "draw", "chart"]
    return any(keyword in question for keyword in drawing_keywords)

# å‘½ä»¤è¡Œäº¤äº’
if __name__ == "__main__":
    main()

def process_terminal_input(query: str) -> str:
    """å¤„ç†ä¸€æ¬¡ç»ˆç«¯è¾“å…¥ï¼Œè¿”å›ç»ˆç«¯é£æ ¼è¾“å‡ºï¼ˆåªä¿ç•™answeréƒ¨åˆ†ï¼‰"""
    global rag
    if rag is None:
        rag = AgenticRAGSystem()
    if query.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
        rag.memory_agent.clear_memory()
        return 'ğŸ§¹ æ­£åœ¨æ¸…ç©ºå¯¹è¯è®°å¿†...\nâœ… å¯¹è¯è®°å¿†å·²æ¸…ç©º\nğŸ‘‹ ç³»ç»Ÿå·²å…³é—­'
    if query.lower() == 'clear':
        rag.memory_agent.clear_memory()
        return 'ğŸ§¹ å¯¹è¯è®°å¿†å·²æ¸…ç©º'
    try:
        result = rag.process_query(query)
        # åªè¿”å›ç»ˆç«¯é£æ ¼çš„ä¸»å›ç­”å†…å®¹
        return result.get('answer', 'æ— å›ç­”')
    except Exception as e:
        return f'âŒ å¤„ç†å¤±è´¥: {str(e)}'

def process_draw_input(query: str) -> str:
    """ä¸“é—¨å¤„ç†ç”»å›¾è¯·æ±‚ï¼Œç»“åˆæ•°æ®åº“æ•°æ®ï¼Œè¿”å›å›¾ç‰‡è·¯å¾„æˆ–é”™è¯¯ä¿¡æ¯"""
    global rag
    if rag is None:
        rag = AgenticRAGSystem()
    try:
        # 1. ç”¨æ•°æ®åº“Agentç”ŸæˆSQL
        sql = rag.db_agent.generate_sql(query)
        plot_data = None
        if sql:
            plot_data = rag.db_agent.get_data_for_plotting(sql)
        db_data_context = ''
        if plot_data and len(plot_data) > 0:
            import json
            db_data_context = json.dumps(plot_data, ensure_ascii=False, indent=2)
        # 2. åªç”¨æ•°æ®åº“æ•°æ®ä½œå›¾
        plot_result = rag.drawing_agent.draw(query, db_data_context)
        if 'æˆåŠŸ' in plot_result and 'æ–‡ä»¶ä¿å­˜åœ¨' in plot_result:
            # æå–å›¾ç‰‡è·¯å¾„
            import re
            m = re.search(r'æ–‡ä»¶ä¿å­˜åœ¨: (.+)', plot_result)
            if m:
                return m.group(1).strip()
        return plot_result
    except Exception as e:
        return f"âŒ ç”»å›¾å¤±è´¥: {str(e)}"


